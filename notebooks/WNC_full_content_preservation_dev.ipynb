{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4f6c9f-0500-4ad5-84bd-f7dddc8cda2f",
   "metadata": {},
   "source": [
    "# Content Preservation Metric Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb364d4-bafb-411d-9b87-af419fc976e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our intended metric is an extension of the Word Movers Distance-based content preservation metric introduced by [Evaluating Style Transfer for Text](https://arxiv.org/pdf/1904.02295.pdf). However, we chose to replace logistic regression for feature importances with word attributions from the trained BERT style classifier. We also replace WMD for cosine similarity on SBert embeddings as it has shown improvements in accuracy and performance ([from blog](https://vaclavkosar.com/ml/Word-Movers-Embedding-Cheap-WMD-For-Documents)).\n",
    "\n",
    "The general approach is to:\n",
    "\n",
    "1. Build style lexicon from trained classifier feature importances\n",
    "2. Use style lexicon to mask out all style tokens in the x and x’ (“removing the style, only content remains”)\n",
    "3. Compare similarity/distance between these two “content only” sentences using cosine similarity between SBert embeddings\n",
    "\n",
    "![](./images/content_preservation_score.jpeg)\n",
    "\n",
    "Two things we need to figure out:\n",
    "1. Does cosine similarity on [masked] SBert embeddings produce meaningful/expected measures of similarity\n",
    "2. How do we precisely mask out __only__ the style related words?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdaf8c50-5f23-4113-a246-289f8d6a8fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a47425-3c44-4667-9e90-1def51590cc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prove SBert works with `<MASK>` tokens in input and similarity metrics make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc00b7a2-7e5c-4e2f-ad2e-eef9b9141815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "# from transformers_interpret import SequenceClassificationExplainer\n",
    "# from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# class ContentPreservationScorer:\n",
    "#     \"\"\"\n",
    "\n",
    "#     Attributes:\n",
    "#         sbert_model_identifier (str)\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, cls_model_identifier: str, sbert_model_identifier: str):\n",
    "\n",
    "#         self.cls_model_identifier = cls_model_identifier\n",
    "#         self.sbert_model_identifier = sbert_model_identifier\n",
    "#         self.device = torch.cuda.current_device() if torch.cuda.is_available() else -1\n",
    "\n",
    "#         self._initialize_hf_artifacts()\n",
    "\n",
    "#     def _initialize_hf_artifacts(self):\n",
    "#         \"\"\"\n",
    "#         Initialize a HuggingFace artifacts (tokenizer and model) according\n",
    "#         to the provided identifiers for both SBert and the classification model.\n",
    "#         Then initialize the word attribution explainer with the HF model+tokenizer.\n",
    "\n",
    "#         \"\"\"\n",
    "\n",
    "#         # sbert\n",
    "#         self.sbert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#             self.sbert_model_identifier\n",
    "#         )\n",
    "#         self.sbert_model = AutoModel.from_pretrained(self.sbert_model_identifier)\n",
    "\n",
    "#         # classifer\n",
    "#         self.cls_tokenizer = AutoTokenizer.from_pretrained(self.cls_model_identifier)\n",
    "#         self.cls_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#             self.cls_model_identifier\n",
    "#         )\n",
    "#         self.cls_model.to(self.device)\n",
    "#         self.explainer = SequenceClassificationExplainer(\n",
    "#             self.cls_model, self.cls_tokenizer\n",
    "#         )\n",
    "\n",
    "#     def compute_sentence_embeddings(self, input_text: List[str]) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Compute sentence embeddings for each sentence provided a list of text strings.\n",
    "\n",
    "#         Args:\n",
    "#             input_text (List[str]) - list of input sentences to encode\n",
    "\n",
    "#         Returns:\n",
    "#             sentence_embeddings (torch.Tensor)\n",
    "\n",
    "#         \"\"\"\n",
    "#         # tokenize sentences\n",
    "#         encoded_input = self.sbert_tokenizer(\n",
    "#             input_text,\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=256,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "\n",
    "#         # to device\n",
    "#         self.sbert_model.eval()\n",
    "#         self.sbert_model.to(self.device)\n",
    "#         encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}\n",
    "\n",
    "#         # compute token embeddings\n",
    "#         with torch.no_grad():\n",
    "#             model_output = self.sbert_model(**encoded_input)\n",
    "\n",
    "#         return (\n",
    "#             self.mean_pooling(model_output, encoded_input[\"attention_mask\"])\n",
    "#             .detach()\n",
    "#             .cpu()\n",
    "#         )\n",
    "\n",
    "#     def calculate_content_preservation_score(self):\n",
    "#         pass\n",
    "\n",
    "#     def calculate_feature_attribution_scores(\n",
    "#         self, text: str, class_index: int = 0, as_norm: bool = False\n",
    "#     ) -> List[tuple]:\n",
    "#         \"\"\"\n",
    "#         Calcualte feature attributions using integrated gradients.\n",
    "\n",
    "#         Args:\n",
    "#             text (str) - text to get attributions for\n",
    "#             class_index (int) - Optional output index to provide attributions for\n",
    "\n",
    "#         \"\"\"\n",
    "#         attributions = self.explainer(text, index=class_index)\n",
    "\n",
    "#         if as_norm:\n",
    "#             idx, values = zip(*attributions)\n",
    "#             df = pd.DataFrame(data=values, index=idx, columns=[\"score\"])\n",
    "#             df[\"abs_norm\"] = df[\"score\"].abs() / df[\"score\"].abs().sum()\n",
    "#             df = df.sort_values(by=\"abs_norm\", ascending=False)\n",
    "#             df[\"cumulative\"] = df[\"abs_norm\"].cumsum()\n",
    "#             return df\n",
    "\n",
    "#         return attributions\n",
    "\n",
    "#     def visualize_feature_attribution_scores(self, text: str, class_index: int = 0):\n",
    "#         \"\"\"\n",
    "#         Calculates and visualizes feature attributions using integrated gradients.\n",
    "\n",
    "#         Args:\n",
    "#             text (str) - text to get attributions for\n",
    "#             class_index (int) - Optional output index to provide attributions for\n",
    "\n",
    "#         \"\"\"\n",
    "#         self.explainer(text, index=class_index)\n",
    "#         self.explainer.visualize()\n",
    "\n",
    "#     @staticmethod\n",
    "#     def cosine_similarity(tensor1: torch.Tensor, tensor2: torch.Tensor) -> List[float]:\n",
    "#         \"\"\" \"\"\"\n",
    "\n",
    "#         assert tensor1.shape == tensor2.shape\n",
    "\n",
    "#         # ensure 2D tensor\n",
    "#         if tensor1.ndim == 1:\n",
    "#             tensor1 = tensor1.unsqueeze(0)\n",
    "#             tensor2 = tensor2.unsqueeze(0)\n",
    "\n",
    "#         cos_sim = torch.nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "#         return cos_sim(tensor1, tensor2).tolist()\n",
    "\n",
    "#     @staticmethod\n",
    "#     def mean_pooling(model_output, attention_mask):\n",
    "#         \"\"\"\n",
    "#         Peform mean pooling over token embeddings to create sentence embedding. Here we take\n",
    "#         the attention mask into account for correct averaging on active token positions.\n",
    "\n",
    "#         CODE BORROWED FROM:\n",
    "#             https://www.sbert.net/examples/applications/computing-embeddings/README.html#sentence-embeddings-with-transformers\n",
    "\n",
    "#         \"\"\"\n",
    "\n",
    "#         token_embeddings = model_output[\n",
    "#             0\n",
    "#         ]  # First element of model_output contains all token embeddings\n",
    "#         input_mask_expanded = (\n",
    "#             attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#         )\n",
    "#         sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "#         sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "#         return sum_embeddings / sum_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1091ea05-26a1-4bdb-a413-455f27fae97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import ContentPreservationScorer\n",
    "\n",
    "SBERT_MODEL_PATH = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CLS_MODEL_PATH = (\n",
    "    \"../models/TRIAL-J-shuffle-lr_3en06-epoch_15-wd_.1-bs_32/checkpoint-67466\"\n",
    ")\n",
    "cps = ContentPreservationScorer(\n",
    "    sbert_model_identifier=SBERT_MODEL_PATH, cls_model_identifier=CLS_MODEL_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8c20727-bed3-4468-af3a-676cced5b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"david crane, born august 10, 1957, is an extremely successful american writer and producer.\"\n",
    "test2 = \"david crane, born august 10, 1957, is an [MASK] [MASK] american writer and producer.\"\n",
    "test3 = \"david crane, born august 10, 1957, is an [MASK] [MASK] producer.\"\n",
    "test4 = \"in 2011 he followed up this debacle with anther rude letter to a constituent.\"\n",
    "test5 = \"he born august 10, 1957, is an [MASK] [MASK] american writer and producer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3ac379-7c97-4c9b-9a21-f012079ac410",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Check that tokenizer handels [MASK] appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdf14bf-a0a9-4eee-ba67-6d33f646b7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2585,\n",
       " 11308,\n",
       " 1010,\n",
       " 2141,\n",
       " 2257,\n",
       " 2184,\n",
       " 1010,\n",
       " 3890,\n",
       " 1010,\n",
       " 2003,\n",
       " 2019,\n",
       " 5186,\n",
       " 3144,\n",
       " 2137,\n",
       " 3213,\n",
       " 1998,\n",
       " 3135,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.sbert_tokenizer.encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbab6ced-bf3c-45ad-91be-70a3a5491b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "103 in cps.sbert_tokenizer.encode(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c77e815-d52d-493e-b13e-f0ca776961c7",
   "metadata": {},
   "source": [
    "#### Check that embedding similarity makes sense with [MASK] tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c131bb1f-09d9-4367-a747-0e0e14e60df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = cps.compute_sentence_embeddings([test, test2, test3, test4, test5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c142b81-87c7-4ab4-a01c-504e7266245e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that embeddings are deterministic with MASKs\n",
    "cps.cosine_similarity(embeddings[0], embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ab76216-e4be-461c-8a12-b2fefa97bfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.6621647477149963], [0.6621647477149963])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if cosine similarity is symmetric\n",
    "cps.cosine_similarity(embeddings[0], embeddings[1]), cps.cosine_similarity(\n",
    "    embeddings[1], embeddings[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cced235-c328-4068-8ae5-8ebe8dcc2e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9872645735740662], [0.06394388526678085])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that test2/test3 are more similar than test2/test4\n",
    "cps.cosine_similarity(embeddings[1], embeddings[2]), cps.cosine_similarity(\n",
    "    embeddings[2], embeddings[3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f84b767-5caa-49cd-a6e8-f461a09a255b",
   "metadata": {},
   "source": [
    "**NOTE:** Looks like the tokenizer can appropriately handle [MASK] special tokens in input text and maintains a reasonable understanding of similarity. Will need to further vet this with real TST examples during evaluation..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84746e1e-6d7b-40a0-bfb2-7d5cc4bcff11",
   "metadata": {},
   "source": [
    "### Key Question: Is style masking or style removal more effective at producing similar embeddings?\n",
    "\n",
    "From the paper: With respect to text modification, style masking may be more suitable as it, on average for WMD, exhibits a higher correlation with human judgments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff64aa0-333f-403f-84e1-70d81b6049a8",
   "metadata": {},
   "source": [
    "#### Style Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92425a3d-31ec-4b4a-bc77-e35a2cfcd293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "xs = \"Harvard is a beautiful school.\"\n",
    "xn = \"Harvard is a school.\"\n",
    "embeddings = cps.compute_sentence_embeddings([xs, xn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "131ee075-b42d-4a68-8711-0dbfbaf68875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8552916049957275]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4133f31-82a0-464c-9dd2-2000279df079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked\n",
    "xs = \"Harvard is a [MASK] school.\"\n",
    "xn = \"Harvard is a school.\"\n",
    "embeddings = cps.compute_sentence_embeddings([xs, xn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60ddaf9f-187f-4040-a17a-87af3b2a5394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7207826972007751]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e81fd253-034f-43e7-98fa-87a1b21d8e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padded???\n",
    "xs = \"Harvard is a [PAD] school.\"\n",
    "xn = \"Harvard is a school.\"\n",
    "embeddings = cps.compute_sentence_embeddings([xs, xn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89af5f9f-950c-4e73-a888-f1db9278bb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9652663469314575]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f93c03-b4b5-4eed-99bf-4c133c6fa500",
   "metadata": {},
   "source": [
    "**Observation:** \n",
    "- It appears that `[MASK]` replacements result in lower similarity than the original... this is not what we want.\n",
    "- In this example, since we just have a single style word, removal will result in an identical sentence with identical embeddings.\n",
    "- It looks like the `[PAD]` (or empty) token actually works pretty well here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e843c6-782b-433d-9cae-b4703ee47728",
   "metadata": {},
   "source": [
    "#### Style Removal\n",
    "\n",
    "What about sentences that are more complex..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61e64cd6-e79a-4778-ba5c-413e97ea649a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9796276092529297]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline\n",
    "xs = \"double indemnity is perhaps the most well known, definitive example of a genre of films called film noir.\"\n",
    "xn = \"double indemnity is an example of a genre of films called film noir.\"\n",
    "embeddings = cps.compute_sentence_embeddings([xs, xn])\n",
    "cps.cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0da33c5-9d1d-4846-94db-68ac182d1f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6714310050010681]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking\n",
    "xs = \"double indemnity is perhaps the [MASK] [MASK] [MASK] [MASK] [MASK] example of a genre of films called film noir.\"\n",
    "xn = \"double indemnity is an example of a genre of films called film noir.\"\n",
    "embeddings = cps.compute_sentence_embeddings([xs, xn])\n",
    "cps.cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4ab0913-41d1-4d9d-9120-5abae47e11bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9909889698028564]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removal\n",
    "xs = \"double indemnity is perhaps the example of a genre of films called film noir.\"\n",
    "xn = \"double indemnity is an example of a genre of films called film noir.\"\n",
    "embeddings = cps.compute_sentence_embeddings([xs, xn])\n",
    "cps.cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f719a7f0-dd4f-4eeb-a472-d91d06e9173e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9467359185218811]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad\n",
    "xs = \"double indemnity is perhaps the [PAD] [PAD] [PAD] [PAD] [PAD] example of a genre of films called film noir.\"\n",
    "xn = \"double indemnity is an example of a genre of films called film noir.\"\n",
    "embeddings = cps.compute_sentence_embeddings([xs, xn])\n",
    "cps.cosine_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aa9eb6-532e-4479-899b-09dbace872ea",
   "metadata": {},
   "source": [
    "Will need to experiment more with both PAD and removal...as of now, looks like removal might better??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe41757-04ab-4336-a4ac-ca9c917dfbc2",
   "metadata": {},
   "source": [
    "## Build style lexicon for the WNC corpus using word attributions from fine-tuned Style Classificaiton transfomer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91ce7b7-2b04-4849-ba06-2ccd1e65e56e",
   "metadata": {},
   "source": [
    "### First, let's get Integrated Gradients working with Captum + Transformers-Interpret\n",
    "\n",
    "**Integrated gradients** is a method originally proposed in Sundararajan et al., “Axiomatic Attribution for Deep Networks” that aims to attribute an importance value to each input feature of a machine learning model based on the gradients of the model output with respect to the input. In particular, integrated gradients defines an attribution value for each feature by considering the integral of the gradients taken along a straight path from a baseline instance  to the input instance.\n",
    "\n",
    "1. First generate a  \"baseline input\" (informationless) of same size as input example. \n",
    "    - For images, all black picture or random pixels. \n",
    "    - For text, a squence of [PAD] tokens. Ex: `[CLS] + [PAD]*(seq_len-2) + [SEP]`\n",
    "2. Then, generate a linear interpolation between the baseline and the original image. You can think of interpolated images as small steps in the feature space between your baseline and input.\n",
    "\n",
    "![](./images/cat_interp.png)\n",
    "\n",
    "3. Calculate gradients at each step between output predictions with respect to input features. Here a step is a process of introducing more and more information with regards to baseline input. N_steps is a tunable parameter (50 - 300)\n",
    "4. Approximate the integral between your baseline and input by accumulating (cumulative average) these local gradients.\n",
    "\n",
    "\n",
    "Integrated Gradients satisfy both of the following principles and thus represent a good attribution method:\n",
    "\n",
    "- Sensitivity: If an input feature changes the classification score in any way, this input should have an attribution value not equal to 0.\n",
    "- Implementation invariance: The result of the attribution should not depend on the design and structure of the neural network. Thus, if two different neural networks provide the same prediction for the same inputs, their attribution values should also be identical.\n",
    "\n",
    "In this case, we are using `captum.attr.LayerIntegratedGradients` as this allows us to specify a specific layers' inputs/features for which we should assign an importance (attribution) score for. The embedding layer is a standard choice since we can not compute attributions for the raw input due to its discrete structure (i.e., we cannot differentiate the output of the model with respect to the discrete input representation). So we calculate attributions of the output with respect to the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83183c42-1350-4889-ad82-92cf2117d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install transformers-interpret\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers_interpret import SequenceClassificationExplainer\n",
    "\n",
    "DEVICE = torch.cuda.current_device() if torch.cuda.is_available() else -1\n",
    "CLS_MODEL_PATH = (\n",
    "    \"../models/TRIAL-J-shuffle-lr_3en06-epoch_15-wd_.1-bs_32/checkpoint-67466\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38bb2fe4-c05e-4564-b2b7-57d8e1837c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(CLS_MODEL_PATH)\n",
    "model.to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CLS_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f78b50d8-b050-4679-a06a-e48c79804730",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"there is an iconic roadhouse, named 'spud's roadhouse', which sells fuel and general shop items , has great meals and has accommodation.\"\n",
    "test = \"chemical abstracts service (cas), a division of the american chemical society, is considered the world's leading source of chemical information.\"\n",
    "test = \"Harvard is a beautiful school.\"\n",
    "test = \" while this subjective assignment of probabilities works well for rolling dice and lotteries, it could also lead to the conclusion that the sun is as likely to rise as to not rise tomorrow morning.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94ab9f2a-64c4-4083-86d8-69d590891382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' while this subjective assignment of probabilities works well for rolling dice and lotteries, it could also lead to the conclusion that the sun is as likely to rise as to not rise tomorrow morning.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "023048c8-c0d7-4ab7-9a5e-e6debe049b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
    "word_attributions = cls_explainer(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6caab62b-2bed-43e0-9fe7-0b4ba081e4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_explainer.predicted_class_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a87420-8550-4753-b0c4-fe9e81f4153e",
   "metadata": {},
   "source": [
    "Positive attribution numbers indicate a word contributes positively towards the predicted class, while negative numbers indicate a word contributes negatively towards the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3bd23a1d-8458-47fc-b53d-04fa352da936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 0.0),\n",
       " ('while', -0.05562276560527483),\n",
       " ('this', 0.31572708215573253),\n",
       " ('subjective', -0.8101973803499191),\n",
       " ('assignment', 0.03173935615089209),\n",
       " ('of', 0.02112331660612022),\n",
       " ('pro', 0.16803625226122712),\n",
       " ('##ba', -0.018405146228170802),\n",
       " ('##bilities', 0.08167633396706857),\n",
       " ('works', 0.0563545170894699),\n",
       " ('well', 0.13795646377426188),\n",
       " ('for', 0.04543423120455286),\n",
       " ('rolling', 0.07429776432569471),\n",
       " ('dice', 0.046204336916178465),\n",
       " ('and', -0.013798630256154106),\n",
       " ('lot', 0.09643428609621504),\n",
       " ('##ter', 0.008971655794682445),\n",
       " ('##ies', 0.01728700645220686),\n",
       " (',', -0.08273477807915612),\n",
       " ('it', 0.07640487502611108),\n",
       " ('could', 0.16356631829091245),\n",
       " ('also', 0.20277085910904877),\n",
       " ('lead', -0.006952292595890877),\n",
       " ('to', 0.09858786597049837),\n",
       " ('the', 0.005357464683234663),\n",
       " ('conclusion', 0.021443037809708222),\n",
       " ('that', -0.009867101889515595),\n",
       " ('the', 0.06205404295747667),\n",
       " ('sun', 0.09670662006933084),\n",
       " ('is', 0.08571196298974545),\n",
       " ('as', 0.0505901646890793),\n",
       " ('likely', 0.07037472959231506),\n",
       " ('to', 0.0545439152286877),\n",
       " ('rise', 0.06020159174558515),\n",
       " ('as', 0.10976302458801213),\n",
       " ('to', 0.0322388592818839),\n",
       " ('not', 0.0900313738529604),\n",
       " ('rise', 0.07883317815992594),\n",
       " ('tomorrow', 0.06798506444529745),\n",
       " ('morning', 0.0503133382192434),\n",
       " ('.', 0.046676334515604115),\n",
       " ('[SEP]', 0.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe84e0-3231-48fb-b9d5-9d5ea43e4967",
   "metadata": {},
   "source": [
    "Sometimes the numeric attributions can be difficult to read particularly in instances where there is a lot of text. To help with that we also provide the visualize() method that utilizes Captum's in built viz library to create a HTML file highlighting the attributions.\n",
    "\n",
    "By default, attributions are always calculated with respect to the predicted class (0 in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7db586a-fc19-481d-9cce-b3b46240c109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1 (0.58)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1</b></text></td><td><text style=\"padding-right:2em\"><b>1.63</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> while                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(0, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> subjective                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> assignment                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pro                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ba                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##bilities                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> works                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> well                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rolling                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dice                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lot                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ter                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ies                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> could                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> also                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lead                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> conclusion                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sun                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> likely                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rise                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> not                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rise                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tomorrow                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> morning                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_explainer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427c4a6-93ce-455c-bd2d-a5148ec2259a",
   "metadata": {},
   "source": [
    "**Explaining Attributions for Non Predicted Class** <br>\n",
    "Attribution explanations are not limited to the predicted class. Let's test a more complex sentence that contains mixed sentiments.\n",
    "\n",
    "In the example below we pass class_index=1 as an argument indicating we would like the attributions to be explained for the NEUTRAL class regardless of what the actual prediction is. Effectively because this is a binary classifier we are getting the inverse attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b53a07ef-cb16-4609-be80-834ae66cc10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 159 ms, sys: 22.3 ms, total: 181 ms\n",
      "Wall time: 180 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cls_explainer_inverse = SequenceClassificationExplainer(model, tokenizer)\n",
    "word_attributions_inverse = cls_explainer_inverse(test, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ec5cb29-cd02-45f7-94e8-b6fb4df5ef87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1 (0.58)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1</b></text></td><td><text style=\"padding-right:2em\"><b>1.63</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> while                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> this                    </font></mark><mark style=\"background-color: hsl(0, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> subjective                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> assignment                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pro                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ba                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##bilities                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> works                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> well                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rolling                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dice                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lot                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ter                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ies                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> could                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> also                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lead                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> conclusion                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sun                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> likely                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rise                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> not                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rise                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> tomorrow                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> morning                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_explainer_inverse.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d2b3ea9-051e-4b3e-8a4f-a0a2ec149568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 0.0),\n",
       " ('while', -0.05562276560527483),\n",
       " ('this', 0.31572708215573253),\n",
       " ('subjective', -0.8101973803499191),\n",
       " ('assignment', 0.03173935615089209),\n",
       " ('of', 0.02112331660612022),\n",
       " ('pro', 0.16803625226122712),\n",
       " ('##ba', -0.018405146228170802),\n",
       " ('##bilities', 0.08167633396706857),\n",
       " ('works', 0.0563545170894699),\n",
       " ('well', 0.13795646377426188),\n",
       " ('for', 0.04543423120455286),\n",
       " ('rolling', 0.07429776432569471),\n",
       " ('dice', 0.046204336916178465),\n",
       " ('and', -0.013798630256154106),\n",
       " ('lot', 0.09643428609621504),\n",
       " ('##ter', 0.008971655794682445),\n",
       " ('##ies', 0.01728700645220686),\n",
       " (',', -0.08273477807915612),\n",
       " ('it', 0.07640487502611108),\n",
       " ('could', 0.16356631829091245),\n",
       " ('also', 0.20277085910904877),\n",
       " ('lead', -0.006952292595890877),\n",
       " ('to', 0.09858786597049837),\n",
       " ('the', 0.005357464683234663),\n",
       " ('conclusion', 0.021443037809708222),\n",
       " ('that', -0.009867101889515595),\n",
       " ('the', 0.06205404295747667),\n",
       " ('sun', 0.09670662006933084),\n",
       " ('is', 0.08571196298974545),\n",
       " ('as', 0.0505901646890793),\n",
       " ('likely', 0.07037472959231506),\n",
       " ('to', 0.0545439152286877),\n",
       " ('rise', 0.06020159174558515),\n",
       " ('as', 0.10976302458801213),\n",
       " ('to', 0.0322388592818839),\n",
       " ('not', 0.0900313738529604),\n",
       " ('rise', 0.07883317815992594),\n",
       " ('tomorrow', 0.06798506444529745),\n",
       " ('morning', 0.0503133382192434),\n",
       " ('.', 0.046676334515604115),\n",
       " ('[SEP]', 0.0)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_attributions_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c447b35-f99f-4b86-a147-77d8d3724d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(30522, 768, padding_idx=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ab628-5e12-4979-97e1-55256b263320",
   "metadata": {},
   "source": [
    "### ~~Build a style lexicon~~\n",
    "\n",
    "Let's assemble a style lexicon from feature attributions using integrated gradients from our BERT model that was trained for text classification on WNC. To do so, we'll:\n",
    "- Aggregate token attribution scores for each token across all sentences in the training dataset (upon which the model was trained).\n",
    "- Attribution scores for each feature will be calculated based on a fixed class target (class 0: subjective) to ensure consistent attribution direction\n",
    "- We will populate the lexicon with features/tokens having the highest absolute average score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2adf80-0cfe-4f88-9fa9-22bb9055bc43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **OLD StyleLexicon Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5a950f48-651f-435c-877a-cb86ec2d9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List\n",
    "# from collections import defaultdict\n",
    "\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# from datasets import load_from_disk\n",
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# from transformers_interpret import SequenceClassificationExplainer\n",
    "\n",
    "\n",
    "# class StyleLexicon:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_identifier: str,\n",
    "#         dataset_identifier: str,\n",
    "#     ):\n",
    "#         self.model_identifier = model_identifier\n",
    "#         self.dataset_identifier = dataset_identifier\n",
    "#         self.device = torch.cuda.current_device() if torch.cuda.is_available() else -1\n",
    "\n",
    "#         self._initialize_hf_artifacts()\n",
    "#         self._construct_dataloader()\n",
    "\n",
    "#     def _initialize_hf_artifacts(self):\n",
    "#         \"\"\"\n",
    "#         Initialize a HuggingFace dataset, tokenizer, and model according\n",
    "#         to the provided identifier. Then initialize the word attribution\n",
    "#         explainer with the huggingface model+tokenizer\n",
    "\n",
    "#         \"\"\"\n",
    "#         self.dataset = load_from_disk(self.dataset_identifier)\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(self.model_identifier)\n",
    "#         self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#             self.model_identifier\n",
    "#         )\n",
    "#         self.model.to(self.device)\n",
    "#         self.explainer = SequenceClassificationExplainer(self.model, self.tokenizer)\n",
    "\n",
    "#     def _construct_dataloader(self):\n",
    "#         \"\"\"\n",
    "#         Initialize the dataloader on train dataset.\n",
    "\n",
    "#         Note: Here we are batching untokenized sentences since the downstream\n",
    "#         SequenceClassificationExplainer operates on raw text. Note batch size of\n",
    "#         just 1 due to limitations of SequenceClassificationExplainer in handling batches.\n",
    "\n",
    "#         \"\"\"\n",
    "#         self.dataloader = torch.utils.data.DataLoader(\n",
    "#             self.dataset[\"train\"],\n",
    "#             batch_size=1,\n",
    "#             drop_last=False,\n",
    "#             pin_memory=True,\n",
    "#             shuffle=False,\n",
    "#         )\n",
    "\n",
    "#     def calculate_feature_attribution_scores(\n",
    "#         self, text: str, class_index: int = 0, as_norm: bool = False\n",
    "#     ) -> List[tuple]:\n",
    "#         \"\"\"\n",
    "#         Calcualte feature attributions using integrated gradients.\n",
    "\n",
    "#         Args:\n",
    "#             text (str) - text to get attributions for\n",
    "#             class_index (int) - Optional output index to provide attributions for\n",
    "\n",
    "#         \"\"\"\n",
    "#         attributions = self.explainer(text, index=class_index)\n",
    "\n",
    "#         if as_norm:\n",
    "#             idx, values = zip(*attributions)\n",
    "#             df = pd.DataFrame(data=values, index=idx, columns=[\"score\"])\n",
    "#             df[\"abs_norm\"] = df[\"score\"].abs() / df[\"score\"].abs().sum()\n",
    "#             df = df.sort_values(by=\"abs_norm\", ascending=False)\n",
    "#             df[\"cumulative\"] = df[\"abs_norm\"].cumsum()\n",
    "#             return df\n",
    "\n",
    "#         return attributions\n",
    "\n",
    "#     def visualize_feature_attribution_scores(self, text: str, class_index: int = 0):\n",
    "#         \"\"\"\n",
    "#         Calculates and visualizes feature attributions using integrated gradients.\n",
    "\n",
    "#         Args:\n",
    "#             text (str) - text to get attributions for\n",
    "#             class_index (int) - Optional output index to provide attributions for\n",
    "\n",
    "#         \"\"\"\n",
    "#         self.explainer(text, index=class_index)\n",
    "#         self.explainer.visualize()\n",
    "\n",
    "#     def collect_feature_attribution_scores(self):\n",
    "\n",
    "#         score_collection = defaultdict(list)\n",
    "\n",
    "#         for i, batch in enumerate(tqdm(self.dataloader)):\n",
    "\n",
    "#             text = batch[\"text\"][0]\n",
    "#             attributions = self.calculate_feature_attribution_scores(text)\n",
    "#             for token, score in attributions:\n",
    "#                 score_collection[token].append(score)\n",
    "\n",
    "#             if i == 5000:\n",
    "#                 break\n",
    "#         return score_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ee0665a0-ff83-4071-8ba2-ee8ba897fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLS_DATASET_PATH = \"/home/cdsw/data/processed/WNC_cls_full\"\n",
    "# CLS_MODEL_PATH = (\n",
    "#     \"../models/TRIAL-J-shuffle-lr_3en06-epoch_15-wd_.1-bs_32/checkpoint-67466\"\n",
    "# )\n",
    "# style_lexicon = StyleLexicon(\n",
    "#     model_identifier=CLS_MODEL_PATH, dataset_identifier=CLS_DATASET_PATH\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ccad6-827e-46d6-9bc9-a94f2bc28a61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Perform \"Token Style Masking\" on a per sentence basis rather than with lexicon dictionary lookup\n",
    "\n",
    "By trying to create a style lexicon, all we really are doing is aggregating high fidelity, local feature importances into low fidelity, global feature importances. This aggregation results in significant information loss, and in fact, kind of negates the power of using a contextual embedding model like BERT in the first place.\n",
    "\n",
    "Instead, we will take advantage of the contextual details and mask/remove tokens on a per sentence basis using word attribution scores as the signal for which tokens to mask/remove. This leaves a few open questions:\n",
    "- What thresholds on word attribution scores for masking?\n",
    "- What threshold on number of words for masking?\n",
    "- Can I develop these based on looking at distributions from train set?\n",
    "\n",
    "\n",
    "**Options:**\n",
    "1. [MEH] Find word attribution score outliers in each sentence using IQR.\n",
    "    - Problem is that most of these won't have an outlier by this definition.\n",
    "    - So, we'd have to change 1.5*IQR to something more lenient (like 1.1*IQR).. this is arbitrary and will be different for each.\n",
    "2. [BAD] Mask top N words proportional to length of sentence\n",
    "    - Problem is that long sentence could have just 1 really strong style word and we'd be masking a number of tokens erroneously just because of length\n",
    "    - Also choosing number of tokens to mask by lenght is arbitrary.\n",
    "3. [MAYBE OK] Mask all tokens that represent up to top X% of style attribution. Or just top token if top token accounts for more than X%.\n",
    "    - Better because it doesn't enforce hard rule on number of tokens to mask\n",
    "    - Rather its however many tokens make up the strongest X% of style attribution\n",
    "    - Problem is still arbitrary determination of that X% threshold...also if no strong style words, then we mask MANY tokens\n",
    "    - Select X to err on the side of precision like the paper --> \"We opt for higher precision and lower recall to minimize the risk of removing content words, which are essential to evaluations of content preservation\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912384a6-bdcb-4b90-b9b7-03fa035f6687",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f494dc3b-ca8c-49f6-87db-58ac0027a71a",
   "metadata": {},
   "source": [
    "#### Ex1\n",
    "\n",
    "- \"contagious\" gets broken up and only the latter parts of the word are considered subejctive...\n",
    "- \"pop\" is considered to be subjective??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8e323fe3-881d-40a6-8c7a-9035e4e40dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1_s = (\n",
    "    \"the band plays an engaging and contagious rhythm known as brega pop and calypso.\"\n",
    ")\n",
    "ex1_n = \"the band plays a rhythm known as brega pop and calypso.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "4ead2885-9b3f-4ab9-920e-101c1af2d5b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0 (0.99)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>1.01</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> band                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> plays                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> an                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> engaging                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> con                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##tag                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ious                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rhythm                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> known                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> br                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ega                    </font></mark><mark style=\"background-color: hsl(120, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pop                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cal                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##yp                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##so                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cps.visualize_feature_attribution_scores(ex1_s)\n",
    "t = cps.calculate_feature_attribution_scores(ex1_s, as_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31c4a3-5ec0-4ff9-a125-d29099b33112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.score.plot(kind=\"box\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "91c1a4d4-fe9f-457a-adba-3ad352066808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9     0.496893\n",
       "8     0.482248\n",
       "15    0.455500\n",
       "2     0.260220\n",
       "10   -0.258252\n",
       "4    -0.237123\n",
       "19    0.164310\n",
       "7    -0.137872\n",
       "3     0.134762\n",
       "12   -0.131099\n",
       "18   -0.114123\n",
       "11   -0.097646\n",
       "20   -0.086629\n",
       "6    -0.058542\n",
       "1     0.043909\n",
       "16    0.041001\n",
       "17   -0.035094\n",
       "13    0.034263\n",
       "5     0.033845\n",
       "14    0.020360\n",
       "0     0.000000\n",
       "21    0.000000\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d650e1ae-0e60-4423-879a-ec498cdb7c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a310a5-fd28-4a4c-be09-8122ee2ceb66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76937cdb-7327-40f1-80c4-598faeae1a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1 (0.01)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>-2.61</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> band                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> plays                    </font></mark><mark style=\"background-color: hsl(0, 75%, 73%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> rhythm                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> known                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> br                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ega                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> pop                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> cal                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##yp                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##so                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>abs_norm</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.686558</td>\n",
       "      <td>0.260588</td>\n",
       "      <td>0.260588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>known</th>\n",
       "      <td>-0.401658</td>\n",
       "      <td>0.152452</td>\n",
       "      <td>0.413040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rhythm</th>\n",
       "      <td>-0.380303</td>\n",
       "      <td>0.144347</td>\n",
       "      <td>0.557386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>band</th>\n",
       "      <td>-0.319601</td>\n",
       "      <td>0.121307</td>\n",
       "      <td>0.678693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.272519</td>\n",
       "      <td>0.103436</td>\n",
       "      <td>0.782129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.117419</td>\n",
       "      <td>0.044567</td>\n",
       "      <td>0.826696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as</th>\n",
       "      <td>-0.110312</td>\n",
       "      <td>0.041869</td>\n",
       "      <td>0.868566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##yp</th>\n",
       "      <td>-0.082348</td>\n",
       "      <td>0.031256</td>\n",
       "      <td>0.899822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##so</th>\n",
       "      <td>-0.070352</td>\n",
       "      <td>0.026703</td>\n",
       "      <td>0.926524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plays</th>\n",
       "      <td>-0.059806</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.949224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.045542</td>\n",
       "      <td>0.017286</td>\n",
       "      <td>0.966510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pop</th>\n",
       "      <td>-0.043753</td>\n",
       "      <td>0.016607</td>\n",
       "      <td>0.983116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cal</th>\n",
       "      <td>-0.029894</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>0.994463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>br</th>\n",
       "      <td>0.010176</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.998325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##ega</th>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[CLS]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[SEP]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           score  abs_norm  cumulative\n",
       "a      -0.686558  0.260588    0.260588\n",
       "known  -0.401658  0.152452    0.413040\n",
       "rhythm -0.380303  0.144347    0.557386\n",
       "band   -0.319601  0.121307    0.678693\n",
       "the    -0.272519  0.103436    0.782129\n",
       "and    -0.117419  0.044567    0.826696\n",
       "as     -0.110312  0.041869    0.868566\n",
       "##yp   -0.082348  0.031256    0.899822\n",
       "##so   -0.070352  0.026703    0.926524\n",
       "plays  -0.059806  0.022700    0.949224\n",
       ".      -0.045542  0.017286    0.966510\n",
       "pop    -0.043753  0.016607    0.983116\n",
       "cal    -0.029894  0.011346    0.994463\n",
       "br      0.010176  0.003862    0.998325\n",
       "##ega   0.004412  0.001675    1.000000\n",
       "[CLS]   0.000000  0.000000    1.000000\n",
       "[SEP]   0.000000  0.000000    1.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.visualize_feature_attribution_scores(ex1_n)\n",
    "cps.calculate_feature_attribution_scores(ex1_n, as_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def16996-fcf2-4bf9-b4d0-1f89b6e93ca9",
   "metadata": {},
   "source": [
    "#### Ex2\n",
    "- Gets prestigious and moving as subjective modifiers.\n",
    "- However, also things \"parents\" is a subjective term..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bac7941a-427c-4524-9f9d-b308b8e9861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex2_s = \"building a difference received the prestigious seal of approval from the parents television council for the moving episode 'page-bailie bunch'.\"\n",
    "ex2_n = \"building a difference received the seal of approval from the parents television council for the episode 'page-bailie bunch'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2d7877a-7776-4cf3-8923-0c97090673b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0 (0.99)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>0.53</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> building                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> difference                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> received                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> prestigious                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seal                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> approval                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> parents                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> television                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> council                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 66%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> moving                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> episode                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> page                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bail                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bunch                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>abs_norm</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>moving</th>\n",
       "      <td>0.685320</td>\n",
       "      <td>0.206718</td>\n",
       "      <td>0.206718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parents</th>\n",
       "      <td>0.400371</td>\n",
       "      <td>0.120767</td>\n",
       "      <td>0.327485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prestigious</th>\n",
       "      <td>0.310053</td>\n",
       "      <td>0.093524</td>\n",
       "      <td>0.421009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.275444</td>\n",
       "      <td>0.083084</td>\n",
       "      <td>0.504093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode</th>\n",
       "      <td>-0.219552</td>\n",
       "      <td>0.066225</td>\n",
       "      <td>0.570318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bunch</th>\n",
       "      <td>-0.168132</td>\n",
       "      <td>0.050715</td>\n",
       "      <td>0.621033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>television</th>\n",
       "      <td>-0.161920</td>\n",
       "      <td>0.048841</td>\n",
       "      <td>0.669874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.135195</td>\n",
       "      <td>0.040780</td>\n",
       "      <td>0.710654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>council</th>\n",
       "      <td>-0.126225</td>\n",
       "      <td>0.038074</td>\n",
       "      <td>0.748728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>received</th>\n",
       "      <td>0.123276</td>\n",
       "      <td>0.037185</td>\n",
       "      <td>0.785913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building</th>\n",
       "      <td>0.112677</td>\n",
       "      <td>0.033988</td>\n",
       "      <td>0.819901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>-0.099878</td>\n",
       "      <td>0.030127</td>\n",
       "      <td>0.850027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##ie</th>\n",
       "      <td>0.072555</td>\n",
       "      <td>0.021885</td>\n",
       "      <td>0.871913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.068854</td>\n",
       "      <td>0.020769</td>\n",
       "      <td>0.892682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.057004</td>\n",
       "      <td>0.017194</td>\n",
       "      <td>0.909876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seal</th>\n",
       "      <td>-0.052862</td>\n",
       "      <td>0.015945</td>\n",
       "      <td>0.925821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <td>0.040025</td>\n",
       "      <td>0.012073</td>\n",
       "      <td>0.937894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bail</th>\n",
       "      <td>0.037899</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>0.949326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'</th>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.010321</td>\n",
       "      <td>0.959647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'</th>\n",
       "      <td>-0.031098</td>\n",
       "      <td>0.009380</td>\n",
       "      <td>0.969028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.030033</td>\n",
       "      <td>0.009059</td>\n",
       "      <td>0.978087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>-0.017548</td>\n",
       "      <td>0.005293</td>\n",
       "      <td>0.983380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difference</th>\n",
       "      <td>-0.016847</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>0.988462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.016058</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>0.993305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>0.015856</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>0.998088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[CLS]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[SEP]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                score  abs_norm  cumulative\n",
       "moving       0.685320  0.206718    0.206718\n",
       "parents      0.400371  0.120767    0.327485\n",
       "prestigious  0.310053  0.093524    0.421009\n",
       "the         -0.275444  0.083084    0.504093\n",
       "episode     -0.219552  0.066225    0.570318\n",
       "bunch       -0.168132  0.050715    0.621033\n",
       "television  -0.161920  0.048841    0.669874\n",
       "the         -0.135195  0.040780    0.710654\n",
       "council     -0.126225  0.038074    0.748728\n",
       "received     0.123276  0.037185    0.785913\n",
       "building     0.112677  0.033988    0.819901\n",
       "for         -0.099878  0.030127    0.850027\n",
       "##ie         0.072555  0.021885    0.871913\n",
       ".            0.068854  0.020769    0.892682\n",
       "a           -0.057004  0.017194    0.909876\n",
       "seal        -0.052862  0.015945    0.925821\n",
       "page         0.040025  0.012073    0.937894\n",
       "bail         0.037899  0.011432    0.949326\n",
       "'            0.034217  0.010321    0.959647\n",
       "'           -0.031098  0.009380    0.969028\n",
       "the         -0.030033  0.009059    0.978087\n",
       "-           -0.017548  0.005293    0.983380\n",
       "difference  -0.016847  0.005082    0.988462\n",
       "of           0.016058  0.004844    0.993305\n",
       "approval     0.015856  0.004783    0.998088\n",
       "from         0.006338  0.001912    1.000000\n",
       "[CLS]        0.000000  0.000000    1.000000\n",
       "[SEP]        0.000000  0.000000    1.000000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.visualize_feature_attribution_scores(ex2_s)\n",
    "cps.calculate_feature_attribution_scores(ex2_s, as_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "815baeb1-ec04-4eae-9b07-4095fb6d27c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1 (0.01)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>-2.70</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> building                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> difference                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> received                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> seal                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> approval                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 76%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> parents                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> television                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> council                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> episode                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> page                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bail                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bunch                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>abs_norm</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>parents</th>\n",
       "      <td>0.495839</td>\n",
       "      <td>0.130589</td>\n",
       "      <td>0.130589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>television</th>\n",
       "      <td>-0.474559</td>\n",
       "      <td>0.124985</td>\n",
       "      <td>0.255573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'</th>\n",
       "      <td>-0.307276</td>\n",
       "      <td>0.080927</td>\n",
       "      <td>0.336501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.302337</td>\n",
       "      <td>0.079626</td>\n",
       "      <td>0.416127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>page</th>\n",
       "      <td>-0.273191</td>\n",
       "      <td>0.071950</td>\n",
       "      <td>0.488077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.220890</td>\n",
       "      <td>0.058176</td>\n",
       "      <td>0.546253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difference</th>\n",
       "      <td>-0.209422</td>\n",
       "      <td>0.055155</td>\n",
       "      <td>0.601408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.195728</td>\n",
       "      <td>0.051549</td>\n",
       "      <td>0.652957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>episode</th>\n",
       "      <td>-0.169463</td>\n",
       "      <td>0.044631</td>\n",
       "      <td>0.697589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>received</th>\n",
       "      <td>-0.148648</td>\n",
       "      <td>0.039149</td>\n",
       "      <td>0.736738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>-0.125017</td>\n",
       "      <td>0.032926</td>\n",
       "      <td>0.769664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.110846</td>\n",
       "      <td>0.029193</td>\n",
       "      <td>0.798857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>council</th>\n",
       "      <td>-0.104559</td>\n",
       "      <td>0.027538</td>\n",
       "      <td>0.826395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.101527</td>\n",
       "      <td>0.026739</td>\n",
       "      <td>0.853134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bail</th>\n",
       "      <td>-0.088971</td>\n",
       "      <td>0.023432</td>\n",
       "      <td>0.876566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approval</th>\n",
       "      <td>-0.077448</td>\n",
       "      <td>0.020397</td>\n",
       "      <td>0.896964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'</th>\n",
       "      <td>-0.076185</td>\n",
       "      <td>0.020065</td>\n",
       "      <td>0.917028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>-0.069595</td>\n",
       "      <td>0.018329</td>\n",
       "      <td>0.935358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bunch</th>\n",
       "      <td>-0.066400</td>\n",
       "      <td>0.017488</td>\n",
       "      <td>0.952845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##ie</th>\n",
       "      <td>-0.055298</td>\n",
       "      <td>0.014564</td>\n",
       "      <td>0.967409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.051862</td>\n",
       "      <td>0.013659</td>\n",
       "      <td>0.981068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>-0.049301</td>\n",
       "      <td>0.012984</td>\n",
       "      <td>0.994053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building</th>\n",
       "      <td>-0.019766</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.999258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seal</th>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.000742</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[CLS]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[SEP]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               score  abs_norm  cumulative\n",
       "parents     0.495839  0.130589    0.130589\n",
       "television -0.474559  0.124985    0.255573\n",
       "'          -0.307276  0.080927    0.336501\n",
       "the        -0.302337  0.079626    0.416127\n",
       "page       -0.273191  0.071950    0.488077\n",
       "the        -0.220890  0.058176    0.546253\n",
       "difference -0.209422  0.055155    0.601408\n",
       "the        -0.195728  0.051549    0.652957\n",
       "episode    -0.169463  0.044631    0.697589\n",
       "received   -0.148648  0.039149    0.736738\n",
       "for        -0.125017  0.032926    0.769664\n",
       "a          -0.110846  0.029193    0.798857\n",
       "council    -0.104559  0.027538    0.826395\n",
       "of         -0.101527  0.026739    0.853134\n",
       "bail       -0.088971  0.023432    0.876566\n",
       "approval   -0.077448  0.020397    0.896964\n",
       "'          -0.076185  0.020065    0.917028\n",
       "from       -0.069595  0.018329    0.935358\n",
       "bunch      -0.066400  0.017488    0.952845\n",
       "##ie       -0.055298  0.014564    0.967409\n",
       ".           0.051862  0.013659    0.981068\n",
       "-          -0.049301  0.012984    0.994053\n",
       "building   -0.019766  0.005206    0.999258\n",
       "seal        0.002816  0.000742    1.000000\n",
       "[CLS]       0.000000  0.000000    1.000000\n",
       "[SEP]       0.000000  0.000000    1.000000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.visualize_feature_attribution_scores(ex2_n)\n",
    "cps.calculate_feature_attribution_scores(ex2_n, as_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eee9de0-39ac-407a-926d-92b61dc86f40",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ex3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5c8836a-0cdd-4f1f-bc7e-155a25d45305",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex3_s = \"water from a fire sprinkler system explosion in 2007\"\n",
    "ex3_n = \"water from a fire sprinkler system failure in 2007\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f6f9d98-5236-4246-8c25-fa41d91edf68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1 (0.37)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>-0.74</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> water                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fire                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sp                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##rin                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##kle                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##r                    </font></mark><mark style=\"background-color: hsl(0, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> system                    </font></mark><mark style=\"background-color: hsl(0, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> explosion                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2007                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>abs_norm</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>explosion</th>\n",
       "      <td>-0.760934</td>\n",
       "      <td>0.315235</td>\n",
       "      <td>0.315235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system</th>\n",
       "      <td>-0.445005</td>\n",
       "      <td>0.184353</td>\n",
       "      <td>0.499588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>-0.342561</td>\n",
       "      <td>0.141914</td>\n",
       "      <td>0.641502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.179300</td>\n",
       "      <td>0.074279</td>\n",
       "      <td>0.715781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>0.146276</td>\n",
       "      <td>0.060598</td>\n",
       "      <td>0.776379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.132378</td>\n",
       "      <td>0.054841</td>\n",
       "      <td>0.831220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>0.130931</td>\n",
       "      <td>0.054241</td>\n",
       "      <td>0.885461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##r</th>\n",
       "      <td>0.083237</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.919944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##kle</th>\n",
       "      <td>0.070713</td>\n",
       "      <td>0.029295</td>\n",
       "      <td>0.949239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##rin</th>\n",
       "      <td>0.057522</td>\n",
       "      <td>0.023830</td>\n",
       "      <td>0.973069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0.036911</td>\n",
       "      <td>0.015291</td>\n",
       "      <td>0.988360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>-0.028098</td>\n",
       "      <td>0.011640</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[CLS]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[SEP]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score  abs_norm  cumulative\n",
       "explosion -0.760934  0.315235    0.315235\n",
       "system    -0.445005  0.184353    0.499588\n",
       "2007      -0.342561  0.141914    0.641502\n",
       "a          0.179300  0.074279    0.715781\n",
       "from       0.146276  0.060598    0.776379\n",
       "water      0.132378  0.054841    0.831220\n",
       "sp         0.130931  0.054241    0.885461\n",
       "##r        0.083237  0.034483    0.919944\n",
       "##kle      0.070713  0.029295    0.949239\n",
       "##rin      0.057522  0.023830    0.973069\n",
       "in         0.036911  0.015291    0.988360\n",
       "fire      -0.028098  0.011640    1.000000\n",
       "[CLS]      0.000000  0.000000    1.000000\n",
       "[SEP]      0.000000  0.000000    1.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.visualize_feature_attribution_scores(ex3_s)\n",
    "cps.calculate_feature_attribution_scores(ex3_s, as_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60cfa0cc-0420-430a-a62f-1e7527fe6a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1 (0.17)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>-1.82</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> water                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> from                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fire                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sp                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##rin                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##kle                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##r                    </font></mark><mark style=\"background-color: hsl(0, 75%, 82%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> system                    </font></mark><mark style=\"background-color: hsl(0, 75%, 74%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> failure                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2007                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>abs_norm</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>failure</th>\n",
       "      <td>-0.651508</td>\n",
       "      <td>0.261295</td>\n",
       "      <td>0.261295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>system</th>\n",
       "      <td>-0.468426</td>\n",
       "      <td>0.187868</td>\n",
       "      <td>0.449163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>-0.375742</td>\n",
       "      <td>0.150696</td>\n",
       "      <td>0.599859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>-0.295798</td>\n",
       "      <td>0.118633</td>\n",
       "      <td>0.718493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>0.263430</td>\n",
       "      <td>0.105652</td>\n",
       "      <td>0.824145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.212420</td>\n",
       "      <td>0.085194</td>\n",
       "      <td>0.909338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##rin</th>\n",
       "      <td>-0.086336</td>\n",
       "      <td>0.034626</td>\n",
       "      <td>0.943964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>-0.056183</td>\n",
       "      <td>0.022533</td>\n",
       "      <td>0.966497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.037709</td>\n",
       "      <td>0.015124</td>\n",
       "      <td>0.981620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>0.026214</td>\n",
       "      <td>0.010513</td>\n",
       "      <td>0.992134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##r</th>\n",
       "      <td>-0.011574</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.996776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##kle</th>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.003224</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[CLS]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[SEP]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score  abs_norm  cumulative\n",
       "failure -0.651508  0.261295    0.261295\n",
       "system  -0.468426  0.187868    0.449163\n",
       "fire    -0.375742  0.150696    0.599859\n",
       "in      -0.295798  0.118633    0.718493\n",
       "2007     0.263430  0.105652    0.824145\n",
       "a       -0.212420  0.085194    0.909338\n",
       "##rin   -0.086336  0.034626    0.943964\n",
       "from    -0.056183  0.022533    0.966497\n",
       "water    0.037709  0.015124    0.981620\n",
       "sp       0.026214  0.010513    0.992134\n",
       "##r     -0.011574  0.004642    0.996776\n",
       "##kle    0.008039  0.003224    1.000000\n",
       "[CLS]    0.000000  0.000000    1.000000\n",
       "[SEP]    0.000000  0.000000    1.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.visualize_feature_attribution_scores(ex3_n)\n",
    "cps.calculate_feature_attribution_scores(ex3_n, as_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb9770b-6169-4ad0-a8d4-e962dedf05be",
   "metadata": {},
   "source": [
    "#### Ex4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "816d5f3d-fca7-4570-b7ce-b14a0749104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex4_s = \"the leela group has commissioned a 1.37 lakh sq ft state of the art building.\"\n",
    "ex4_n = \"the leela group has commissioned a 1.37 lakh sq ft building.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3a9a871-a66b-45e1-8641-2cd9850b29a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0 (0.99)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>0.26</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lee                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##la                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> group                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(120, 75%, 83%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> commissioned                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 37                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> la                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##kh                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sq                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ft                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> state                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> art                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> building                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>abs_norm</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>art</th>\n",
       "      <td>0.736597</td>\n",
       "      <td>0.255327</td>\n",
       "      <td>0.255327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commissioned</th>\n",
       "      <td>0.354312</td>\n",
       "      <td>0.122815</td>\n",
       "      <td>0.378142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>0.333868</td>\n",
       "      <td>0.115729</td>\n",
       "      <td>0.493871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.278532</td>\n",
       "      <td>0.096547</td>\n",
       "      <td>0.590418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <td>-0.215907</td>\n",
       "      <td>0.074840</td>\n",
       "      <td>0.665258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.170277</td>\n",
       "      <td>0.059023</td>\n",
       "      <td>0.724281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.155104</td>\n",
       "      <td>0.053764</td>\n",
       "      <td>0.778045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>building</th>\n",
       "      <td>-0.120421</td>\n",
       "      <td>0.041742</td>\n",
       "      <td>0.819786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>-0.090933</td>\n",
       "      <td>0.031520</td>\n",
       "      <td>0.851306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sq</th>\n",
       "      <td>-0.075807</td>\n",
       "      <td>0.026277</td>\n",
       "      <td>0.877584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##kh</th>\n",
       "      <td>-0.054935</td>\n",
       "      <td>0.019042</td>\n",
       "      <td>0.896626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>la</th>\n",
       "      <td>0.052711</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.914897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lee</th>\n",
       "      <td>-0.049272</td>\n",
       "      <td>0.017079</td>\n",
       "      <td>0.931977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.043451</td>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.947038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.038386</td>\n",
       "      <td>0.013306</td>\n",
       "      <td>0.960344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.035312</td>\n",
       "      <td>0.012240</td>\n",
       "      <td>0.972584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ft</th>\n",
       "      <td>0.029317</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.982746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.024507</td>\n",
       "      <td>0.008495</td>\n",
       "      <td>0.991241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##la</th>\n",
       "      <td>-0.017352</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.997256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.007917</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[CLS]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[SEP]</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 score  abs_norm  cumulative\n",
       "art           0.736597  0.255327    0.255327\n",
       "commissioned  0.354312  0.122815    0.378142\n",
       "state         0.333868  0.115729    0.493871\n",
       "the          -0.278532  0.096547    0.590418\n",
       "group        -0.215907  0.074840    0.665258\n",
       "a            -0.170277  0.059023    0.724281\n",
       ".            -0.155104  0.053764    0.778045\n",
       "building     -0.120421  0.041742    0.819786\n",
       "has          -0.090933  0.031520    0.851306\n",
       "sq           -0.075807  0.026277    0.877584\n",
       "##kh         -0.054935  0.019042    0.896626\n",
       "la            0.052711  0.018271    0.914897\n",
       "lee          -0.049272  0.017079    0.931977\n",
       ".             0.043451  0.015061    0.947038\n",
       "1            -0.038386  0.013306    0.960344\n",
       "the          -0.035312  0.012240    0.972584\n",
       "ft            0.029317  0.010162    0.982746\n",
       "of            0.024507  0.008495    0.991241\n",
       "##la         -0.017352  0.006015    0.997256\n",
       "37           -0.007917  0.002744    1.000000\n",
       "[CLS]         0.000000  0.000000    1.000000\n",
       "[SEP]         0.000000  0.000000    1.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.visualize_feature_attribution_scores(ex4_s)\n",
    "cps.calculate_feature_attribution_scores(ex4_s, as_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "34ab1654-d0ef-47bb-9866-aa254554faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1 (0.03)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>-2.08</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> lee                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##la                    </font></mark><mark style=\"background-color: hsl(0, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> group                    </font></mark><mark style=\"background-color: hsl(0, 75%, 80%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> commissioned                    </font></mark><mark style=\"background-color: hsl(0, 75%, 78%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 1                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 37                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> la                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##kh                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sq                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ft                    </font></mark><mark style=\"background-color: hsl(0, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> building                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>score</th>\n",
       "      <th>abs_norm</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a</td>\n",
       "      <td>-0.553373</td>\n",
       "      <td>0.188336</td>\n",
       "      <td>0.188336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>has</td>\n",
       "      <td>-0.506389</td>\n",
       "      <td>0.172346</td>\n",
       "      <td>0.360682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>group</td>\n",
       "      <td>-0.372519</td>\n",
       "      <td>0.126784</td>\n",
       "      <td>0.487466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>building</td>\n",
       "      <td>-0.342775</td>\n",
       "      <td>0.116661</td>\n",
       "      <td>0.604127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>commissioned</td>\n",
       "      <td>0.277783</td>\n",
       "      <td>0.094541</td>\n",
       "      <td>0.698668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>-0.212587</td>\n",
       "      <td>0.072352</td>\n",
       "      <td>0.771020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##la</td>\n",
       "      <td>-0.131129</td>\n",
       "      <td>0.044629</td>\n",
       "      <td>0.815649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sq</td>\n",
       "      <td>-0.107308</td>\n",
       "      <td>0.036522</td>\n",
       "      <td>0.852171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lee</td>\n",
       "      <td>-0.095123</td>\n",
       "      <td>0.032375</td>\n",
       "      <td>0.884545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>##kh</td>\n",
       "      <td>-0.085631</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>0.913689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>la</td>\n",
       "      <td>0.072350</td>\n",
       "      <td>0.024624</td>\n",
       "      <td>0.938313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ft</td>\n",
       "      <td>-0.070892</td>\n",
       "      <td>0.024127</td>\n",
       "      <td>0.962441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>.</td>\n",
       "      <td>0.037605</td>\n",
       "      <td>0.012798</td>\n",
       "      <td>0.975239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>37</td>\n",
       "      <td>-0.031617</td>\n",
       "      <td>0.010761</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>.</td>\n",
       "      <td>0.028201</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>0.995598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.012935</td>\n",
       "      <td>0.004402</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           token     score  abs_norm  cumulative\n",
       "7              a -0.553373  0.188336    0.188336\n",
       "5            has -0.506389  0.172346    0.360682\n",
       "4          group -0.372519  0.126784    0.487466\n",
       "15      building -0.342775  0.116661    0.604127\n",
       "6   commissioned  0.277783  0.094541    0.698668\n",
       "1            the -0.212587  0.072352    0.771020\n",
       "3           ##la -0.131129  0.044629    0.815649\n",
       "13            sq -0.107308  0.036522    0.852171\n",
       "2            lee -0.095123  0.032375    0.884545\n",
       "12          ##kh -0.085631  0.029144    0.913689\n",
       "11            la  0.072350  0.024624    0.938313\n",
       "14            ft -0.070892  0.024127    0.962441\n",
       "16             .  0.037605  0.012798    0.975239\n",
       "10            37 -0.031617  0.010761    0.986000\n",
       "9              .  0.028201  0.009598    0.995598\n",
       "8              1  0.012935  0.004402    1.000000\n",
       "0          [CLS]  0.000000  0.000000    1.000000\n",
       "17         [SEP]  0.000000  0.000000    1.000000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.visualize_feature_attribution_scores(ex4_n)\n",
    "cps.calculate_feature_attribution_scores(ex4_n, as_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b9a3d-6dfc-4367-a23d-b4c18fc5087b",
   "metadata": {},
   "source": [
    "#### Ex4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "9d3043ed-06aa-4c54-bac0-b2d23959cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex5_s = \"singapore has a highly developed and successful market economy.\"\n",
    "ex5_n = \"singapore has a highly developed trade-oriented market economy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "adcf4449-1f34-45b5-909e-4ee9a95e6551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0 (0.69)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>0.11</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> singapore                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> highly                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> developed                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 68%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> successful                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> market                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> economy                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>score</th>\n",
       "      <th>abs_norm</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>successful</td>\n",
       "      <td>-0.808754</td>\n",
       "      <td>0.376367</td>\n",
       "      <td>0.376367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>has</td>\n",
       "      <td>0.337286</td>\n",
       "      <td>0.156962</td>\n",
       "      <td>0.533328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>singapore</td>\n",
       "      <td>0.323927</td>\n",
       "      <td>0.150745</td>\n",
       "      <td>0.684073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>economy</td>\n",
       "      <td>0.278802</td>\n",
       "      <td>0.129745</td>\n",
       "      <td>0.813818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>highly</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>0.894219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>developed</td>\n",
       "      <td>0.125653</td>\n",
       "      <td>0.058475</td>\n",
       "      <td>0.952693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>and</td>\n",
       "      <td>0.054474</td>\n",
       "      <td>0.025350</td>\n",
       "      <td>0.978043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>-0.025629</td>\n",
       "      <td>0.011927</td>\n",
       "      <td>0.989970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>.</td>\n",
       "      <td>-0.011578</td>\n",
       "      <td>0.005388</td>\n",
       "      <td>0.995358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>market</td>\n",
       "      <td>0.009975</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token     score  abs_norm  cumulative\n",
       "7   successful -0.808754  0.376367    0.376367\n",
       "2          has  0.337286  0.156962    0.533328\n",
       "1    singapore  0.323927  0.150745    0.684073\n",
       "9      economy  0.278802  0.129745    0.813818\n",
       "4       highly -0.172768  0.080400    0.894219\n",
       "5    developed  0.125653  0.058475    0.952693\n",
       "6          and  0.054474  0.025350    0.978043\n",
       "3            a -0.025629  0.011927    0.989970\n",
       "10           . -0.011578  0.005388    0.995358\n",
       "8       market  0.009975  0.004642    1.000000\n",
       "0        [CLS]  0.000000  0.000000    1.000000\n",
       "11       [SEP]  0.000000  0.000000    1.000000"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.visualize_feature_attribution_scores(ex5_s)\n",
    "cps.calculate_feature_attribution_scores(ex5_s, as_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "216f3404-2147-4a18-ac1c-eedc9e5fa196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_1 (0.27)</b></text></td><td><text style=\"padding-right:2em\"><b>LABEL_0</b></text></td><td><text style=\"padding-right:2em\"><b>-1.06</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> singapore                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 64%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> highly                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> developed                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> trade                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> oriented                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> market                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> economy                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>score</th>\n",
       "      <th>abs_norm</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>highly</td>\n",
       "      <td>-0.924491</td>\n",
       "      <td>0.479649</td>\n",
       "      <td>0.479649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>trade</td>\n",
       "      <td>-0.207859</td>\n",
       "      <td>0.107842</td>\n",
       "      <td>0.587491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>singapore</td>\n",
       "      <td>0.183225</td>\n",
       "      <td>0.095062</td>\n",
       "      <td>0.682553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>market</td>\n",
       "      <td>-0.144004</td>\n",
       "      <td>0.074713</td>\n",
       "      <td>0.757266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>has</td>\n",
       "      <td>0.131890</td>\n",
       "      <td>0.068428</td>\n",
       "      <td>0.825694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>oriented</td>\n",
       "      <td>-0.113468</td>\n",
       "      <td>0.058870</td>\n",
       "      <td>0.884563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>developed</td>\n",
       "      <td>-0.104355</td>\n",
       "      <td>0.054142</td>\n",
       "      <td>0.938706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>economy</td>\n",
       "      <td>0.077393</td>\n",
       "      <td>0.040154</td>\n",
       "      <td>0.978859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-</td>\n",
       "      <td>0.019251</td>\n",
       "      <td>0.009988</td>\n",
       "      <td>0.988847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>0.015799</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.997044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token     score  abs_norm  cumulative\n",
       "4      highly -0.924491  0.479649    0.479649\n",
       "6       trade -0.207859  0.107842    0.587491\n",
       "1   singapore  0.183225  0.095062    0.682553\n",
       "9      market -0.144004  0.074713    0.757266\n",
       "2         has  0.131890  0.068428    0.825694\n",
       "8    oriented -0.113468  0.058870    0.884563\n",
       "5   developed -0.104355  0.054142    0.938706\n",
       "10    economy  0.077393  0.040154    0.978859\n",
       "7           -  0.019251  0.009988    0.988847\n",
       "3           a  0.015799  0.008197    0.997044\n",
       "11          .  0.005698  0.002956    1.000000\n",
       "0       [CLS]  0.000000  0.000000    1.000000\n",
       "12      [SEP]  0.000000  0.000000    1.000000"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.visualize_feature_attribution_scores(ex5_n)\n",
    "cps.calculate_feature_attribution_scores(ex5_n, as_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7d9a3-a003-4a6b-9eef-adddd3298800",
   "metadata": {},
   "source": [
    "**Open Questions**\n",
    "1. Does style token __masking__ or __removal__ retain more meaningful sentence embeddings for similarity?\n",
    "2. How do we determine which tokens should be masked/removed from each subjective/neutral sentence?\n",
    "    - What thresholds on word attribution scores?\n",
    "    - What threshold on number of words?\n",
    "    - Can I develop these based on looking at distributions from train set?\n",
    "3. Is there any good way to quantitatively assess these tradeoffs other than manual inspection? It seems like no...\n",
    "    - The only way I could think is to draw correlations between automated metrics and human evaluation like the paper does, but that requires human evaluation...\n",
    "    - We also can draw high level conclusions by comparing scores against the ground truth annotation. For example, CPS (averaged across the test dataset) on input -> generated text should score worse than CPS on input -> ground truth annotation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9762a85-1c19-4b51-beaa-ebb543879265",
   "metadata": {},
   "source": [
    "### Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7f94b7b5-743b-4f81-a2d6-c7dff55274a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import ContentPreservationScorer\n",
    "\n",
    "SBERT_MODEL_PATH = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CLS_MODEL_PATH = (\n",
    "    \"../models/TRIAL-J-shuffle-lr_3en06-epoch_15-wd_.1-bs_32/checkpoint-67466\"\n",
    ")\n",
    "cps = ContentPreservationScorer(\n",
    "    sbert_model_identifier=SBERT_MODEL_PATH, cls_model_identifier=CLS_MODEL_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "9145e54e-a1b6-435b-8cdd-9e0257442e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'water from a fire sprinkler system failure in 2007'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex3_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "37d503aa-6fd5-4cae-8036-0cb9263b8ff9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'water from a fire sprinkler [PAD] [PAD] in 2007'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.mask_style_tokens(ex3_n, threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "96c72680-6f2f-47ae-af94-162e48f7d470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 0.0),\n",
       " ('water', 0.03770883064536592),\n",
       " ('from', -0.056182666589296305),\n",
       " ('a', -0.21242018307798347),\n",
       " ('fire', -0.3757421639042086),\n",
       " ('sp', 0.026213781289581465),\n",
       " ('##rin', -0.08633557562434407),\n",
       " ('##kle', 0.008039334304876683),\n",
       " ('##r', -0.011573948649361515),\n",
       " ('system', -0.4684262250666483),\n",
       " ('failure', -0.6515077664214646),\n",
       " ('in', -0.295798079019804),\n",
       " ('2007', 0.26343018321977363),\n",
       " ('[SEP]', 0.0)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.calculate_feature_attribution_scores(ex3_n, as_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "926089ae-2788-4eba-8f71-a689f19625cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>score</th>\n",
       "      <th>abs_norm</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>failure</td>\n",
       "      <td>-0.651508</td>\n",
       "      <td>0.261295</td>\n",
       "      <td>0.261295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>system</td>\n",
       "      <td>-0.468426</td>\n",
       "      <td>0.187868</td>\n",
       "      <td>0.449163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fire</td>\n",
       "      <td>-0.375742</td>\n",
       "      <td>0.150696</td>\n",
       "      <td>0.599859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>in</td>\n",
       "      <td>-0.295798</td>\n",
       "      <td>0.118633</td>\n",
       "      <td>0.718493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2007</td>\n",
       "      <td>0.263430</td>\n",
       "      <td>0.105652</td>\n",
       "      <td>0.824145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>-0.212420</td>\n",
       "      <td>0.085194</td>\n",
       "      <td>0.909338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>##rin</td>\n",
       "      <td>-0.086336</td>\n",
       "      <td>0.034626</td>\n",
       "      <td>0.943964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from</td>\n",
       "      <td>-0.056183</td>\n",
       "      <td>0.022533</td>\n",
       "      <td>0.966497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>water</td>\n",
       "      <td>0.037709</td>\n",
       "      <td>0.015124</td>\n",
       "      <td>0.981620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sp</td>\n",
       "      <td>0.026214</td>\n",
       "      <td>0.010513</td>\n",
       "      <td>0.992134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>##r</td>\n",
       "      <td>-0.011574</td>\n",
       "      <td>0.004642</td>\n",
       "      <td>0.996776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>##kle</td>\n",
       "      <td>0.008039</td>\n",
       "      <td>0.003224</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      token     score  abs_norm  cumulative\n",
       "10  failure -0.651508  0.261295    0.261295\n",
       "9    system -0.468426  0.187868    0.449163\n",
       "4      fire -0.375742  0.150696    0.599859\n",
       "11       in -0.295798  0.118633    0.718493\n",
       "12     2007  0.263430  0.105652    0.824145\n",
       "3         a -0.212420  0.085194    0.909338\n",
       "6     ##rin -0.086336  0.034626    0.943964\n",
       "2      from -0.056183  0.022533    0.966497\n",
       "1     water  0.037709  0.015124    0.981620\n",
       "5        sp  0.026214  0.010513    0.992134\n",
       "8       ##r -0.011574  0.004642    0.996776\n",
       "7     ##kle  0.008039  0.003224    1.000000\n",
       "0     [CLS]  0.000000  0.000000    1.000000\n",
       "13    [SEP]  0.000000  0.000000    1.000000"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.calculate_feature_attribution_scores(ex3_n, as_norm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48975e-e5f7-44b9-a6a3-944636c12160",
   "metadata": {},
   "source": [
    "Experimentally determine the best value parameters:\n",
    "\n",
    "0. implement the ability to remove tokens along with [PAD]-ing them\n",
    "1. Search through values of `threshold` (0 --> 0.5 by 0.05, maybe?) and `mask_type` ([PAD] vs. removal) to see how many tokens get removed on average (other stats)\n",
    "2. For each threshold, calculate downstream impact on cosine similarity on sentence pairs (CPS score) -- use that to compare against baseline (no removal). See which threshold value (if any) produce \"better\" similarity scores.\n",
    "3. During the experiments collect metric for number of tokens were masked that were \"sub-tokens\" (i.e. start with ##)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bcb1b-e5cc-466d-a33e-17b761095f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a984a-0d6e-453e-9dc0-87f9d7ca75d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea40b05-539e-468e-a81f-46c48b787a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592a08e-5712-4723-a90c-a8276197446e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70c8adb-7f92-4593-978f-b26f81cc9e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b834d04-36af-48bd-b807-d89dfd0e8ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893e1698-5bd8-4036-a737-eb327b74517c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8edca2-0720-4784-9354-cd9581d4068a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec43a9a-db65-47c1-8556-b7a5b7c08f3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334eac5b-fb3c-4619-9b53-6865cefba604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6dedc-f550-4f19-baed-f0b282798c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "14ca8d1f-d220-4964-85af-7230bc50810d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "token_idxs_to_mask = []\n",
    "\n",
    "# If the first token accounts for more than the set\n",
    "# threshold, take just that token to maks. Otherwise,\n",
    "# take all tokens up to the threshold\n",
    "if df.iloc[0][\"cumulative\"] > threshold:\n",
    "    token_idxs_to_mask.append(df.index[0])\n",
    "else:\n",
    "    token_idxs_to_mask.extend(df[df[\"cumulative\"] <= threshold].index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "af086482-0f8a-4086-b813-e5d3fd33d8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_idxs_to_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5ce31911-d2ed-498d-a1df-8a224d3bb490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18833620693903458"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"cumulative\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4b6200-fe5c-4292-8d9d-33398a38eacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6fd1c97b-7b7b-4c9a-bd4a-b2e5656a322c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Float64Index([0.18833620693903458, 0.36068194656942054, 0.48746589634617643,\\n               0.6041266845545876,  0.6986679687665844,  0.7710204654610139,\\n               0.8156492233566381,  0.8521707299497359,  0.8845452518432344,\\n               0.9136892231565761,  0.9383130522484219,  0.9624405204513498,\\n               0.9752389578534958,  0.9859995678527124,  0.9955977358364337,\\n               0.9999999999999999,  0.9999999999999999,  0.9999999999999999],\\n             dtype='float64')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-fcbd8d9b1af6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cumulative\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3510\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3511\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3512\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3514\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5780\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5782\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5784\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5840\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5841\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5842\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5844\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Float64Index([0.18833620693903458, 0.36068194656942054, 0.48746589634617643,\\n               0.6041266845545876,  0.6986679687665844,  0.7710204654610139,\\n               0.8156492233566381,  0.8521707299497359,  0.8845452518432344,\\n               0.9136892231565761,  0.9383130522484219,  0.9624405204513498,\\n               0.9752389578534958,  0.9859995678527124,  0.9955977358364337,\\n               0.9999999999999999,  0.9999999999999999,  0.9999999999999999],\\n             dtype='float64')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "df[df[\"cumulative\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7f8b9843-bf90-4dc1-92dd-fc1801a2abc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 5]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"cumulative\"] <= 0.4].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333c1ed1-e38a-4220-8bac-32d89a47beb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b502bf80-2d69-492e-b1ed-e353e1fcf3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5000/308394 [11:54<12:02:29,  7.00it/s]\n"
     ]
    }
   ],
   "source": [
    "sc = style_lexicon.collect_feature_attribution_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0df2139-9310-49ee-bb56-563e9444a8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = defaultdict()\n",
    "for token, scores in sc.items():\n",
    "    mean_scores[token] = np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f7c9ef8f-57f0-4add-99fb-51d07bcd1c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores_df = pd.DataFrame.from_dict(\n",
    "    mean_scores, orient=\"index\", columns=[\"score\"]\n",
    ").sort_values(by=\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "989cb890-49f6-48fc-9545-ae768b93c452",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>maybe</th>\n",
       "      <td>-0.976758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tender</th>\n",
       "      <td>-0.903334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>integral</th>\n",
       "      <td>-0.886136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cheap</th>\n",
       "      <td>-0.881585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>truly</th>\n",
       "      <td>-0.869142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conceal</th>\n",
       "      <td>-0.831310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fen</th>\n",
       "      <td>-0.828528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##ower</th>\n",
       "      <td>-0.821035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jeanne</th>\n",
       "      <td>-0.809552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strides</th>\n",
       "      <td>-0.807676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proposing</th>\n",
       "      <td>-0.800291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>questioned</th>\n",
       "      <td>-0.799037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topped</th>\n",
       "      <td>-0.794311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amended</th>\n",
       "      <td>-0.792447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>labeled</th>\n",
       "      <td>-0.787601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consistently</th>\n",
       "      <td>-0.787181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>-0.782363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simultaneous</th>\n",
       "      <td>-0.775198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tail</th>\n",
       "      <td>-0.774805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##dot</th>\n",
       "      <td>-0.771106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amalgamated</th>\n",
       "      <td>-0.760501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seemed</th>\n",
       "      <td>-0.754041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eliot</th>\n",
       "      <td>-0.739788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nato</th>\n",
       "      <td>-0.737881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rebellious</th>\n",
       "      <td>-0.734047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surpassed</th>\n",
       "      <td>-0.731902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##tained</th>\n",
       "      <td>-0.728326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brush</th>\n",
       "      <td>-0.712062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attained</th>\n",
       "      <td>-0.709816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>silent</th>\n",
       "      <td>-0.707855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sharply</th>\n",
       "      <td>-0.707745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>echo</th>\n",
       "      <td>-0.702814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>efficiency</th>\n",
       "      <td>-0.702705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##vich</th>\n",
       "      <td>-0.693724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscure</th>\n",
       "      <td>-0.684208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>##uram</th>\n",
       "      <td>-0.683203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>proprietary</th>\n",
       "      <td>-0.679703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>specialized</th>\n",
       "      <td>-0.677925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scenic</th>\n",
       "      <td>-0.657824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intentionally</th>\n",
       "      <td>-0.655385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contend</th>\n",
       "      <td>-0.655202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innocence</th>\n",
       "      <td>-0.652253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evolving</th>\n",
       "      <td>-0.649208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rough</th>\n",
       "      <td>-0.647915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoil</th>\n",
       "      <td>-0.646354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interpret</th>\n",
       "      <td>-0.643595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buffer</th>\n",
       "      <td>-0.642982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comfortable</th>\n",
       "      <td>-0.642604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conquered</th>\n",
       "      <td>-0.642530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accidentally</th>\n",
       "      <td>-0.637010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  score\n",
       "maybe         -0.976758\n",
       "tender        -0.903334\n",
       "integral      -0.886136\n",
       "cheap         -0.881585\n",
       "truly         -0.869142\n",
       "conceal       -0.831310\n",
       "fen           -0.828528\n",
       "##ower        -0.821035\n",
       "jeanne        -0.809552\n",
       "strides       -0.807676\n",
       "proposing     -0.800291\n",
       "questioned    -0.799037\n",
       "topped        -0.794311\n",
       "amended       -0.792447\n",
       "labeled       -0.787601\n",
       "consistently  -0.787181\n",
       "toxic         -0.782363\n",
       "simultaneous  -0.775198\n",
       "tail          -0.774805\n",
       "##dot         -0.771106\n",
       "amalgamated   -0.760501\n",
       "seemed        -0.754041\n",
       "eliot         -0.739788\n",
       "nato          -0.737881\n",
       "rebellious    -0.734047\n",
       "surpassed     -0.731902\n",
       "##tained      -0.728326\n",
       "brush         -0.712062\n",
       "attained      -0.709816\n",
       "silent        -0.707855\n",
       "sharply       -0.707745\n",
       "echo          -0.702814\n",
       "efficiency    -0.702705\n",
       "##vich        -0.693724\n",
       "obscure       -0.684208\n",
       "##uram        -0.683203\n",
       "proprietary   -0.679703\n",
       "specialized   -0.677925\n",
       "scenic        -0.657824\n",
       "intentionally -0.655385\n",
       "contend       -0.655202\n",
       "innocence     -0.652253\n",
       "evolving      -0.649208\n",
       "rough         -0.647915\n",
       "spoil         -0.646354\n",
       "interpret     -0.643595\n",
       "buffer        -0.642982\n",
       "comfortable   -0.642604\n",
       "conquered     -0.642530\n",
       "accidentally  -0.637010"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e9f7ac56-216c-491b-ac0b-12b8eaa790f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intriguing</th>\n",
       "      <td>0.933927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thumping</th>\n",
       "      <td>0.934078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ridiculous</th>\n",
       "      <td>0.934748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>succumbed</th>\n",
       "      <td>0.934966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonsense</th>\n",
       "      <td>0.935777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coveted</th>\n",
       "      <td>0.936367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prestigious</th>\n",
       "      <td>0.937255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adored</th>\n",
       "      <td>0.939600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enjoys</th>\n",
       "      <td>0.939777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>invincible</th>\n",
       "      <td>0.939937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deserted</th>\n",
       "      <td>0.940183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illusions</th>\n",
       "      <td>0.940693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valiant</th>\n",
       "      <td>0.940759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affordable</th>\n",
       "      <td>0.941139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horrors</th>\n",
       "      <td>0.941560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wrongly</th>\n",
       "      <td>0.944224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hated</th>\n",
       "      <td>0.945799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incredible</th>\n",
       "      <td>0.947372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pleasing</th>\n",
       "      <td>0.947657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>magnificent</th>\n",
       "      <td>0.948040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strangely</th>\n",
       "      <td>0.948105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprisingly</th>\n",
       "      <td>0.949018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delight</th>\n",
       "      <td>0.949085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>famed</th>\n",
       "      <td>0.951808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delightful</th>\n",
       "      <td>0.953848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>splendid</th>\n",
       "      <td>0.954550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instantly</th>\n",
       "      <td>0.954834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flawless</th>\n",
       "      <td>0.955628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boasts</th>\n",
       "      <td>0.956956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lawful</th>\n",
       "      <td>0.956981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desperately</th>\n",
       "      <td>0.956998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>picturesque</th>\n",
       "      <td>0.960807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizard</th>\n",
       "      <td>0.961366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>staggering</th>\n",
       "      <td>0.963010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foolish</th>\n",
       "      <td>0.963469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sublime</th>\n",
       "      <td>0.963824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treacherous</th>\n",
       "      <td>0.964167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eager</th>\n",
       "      <td>0.964345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sensation</th>\n",
       "      <td>0.964479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>honest</th>\n",
       "      <td>0.965951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>horribly</th>\n",
       "      <td>0.966833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>illustrious</th>\n",
       "      <td>0.966942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>luxurious</th>\n",
       "      <td>0.968997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rogue</th>\n",
       "      <td>0.972301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hilarious</th>\n",
       "      <td>0.974727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>clever</th>\n",
       "      <td>0.974823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expansive</th>\n",
       "      <td>0.975288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pork</th>\n",
       "      <td>0.978309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brutally</th>\n",
       "      <td>0.979251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hopefully</th>\n",
       "      <td>0.988199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 score\n",
       "intriguing    0.933927\n",
       "thumping      0.934078\n",
       "ridiculous    0.934748\n",
       "succumbed     0.934966\n",
       "nonsense      0.935777\n",
       "coveted       0.936367\n",
       "prestigious   0.937255\n",
       "adored        0.939600\n",
       "enjoys        0.939777\n",
       "invincible    0.939937\n",
       "deserted      0.940183\n",
       "illusions     0.940693\n",
       "valiant       0.940759\n",
       "affordable    0.941139\n",
       "horrors       0.941560\n",
       "wrongly       0.944224\n",
       "hated         0.945799\n",
       "incredible    0.947372\n",
       "pleasing      0.947657\n",
       "magnificent   0.948040\n",
       "strangely     0.948105\n",
       "surprisingly  0.949018\n",
       "delight       0.949085\n",
       "famed         0.951808\n",
       "delightful    0.953848\n",
       "splendid      0.954550\n",
       "instantly     0.954834\n",
       "flawless      0.955628\n",
       "boasts        0.956956\n",
       "lawful        0.956981\n",
       "desperately   0.956998\n",
       "picturesque   0.960807\n",
       "wizard        0.961366\n",
       "staggering    0.963010\n",
       "foolish       0.963469\n",
       "sublime       0.963824\n",
       "treacherous   0.964167\n",
       "eager         0.964345\n",
       "sensation     0.964479\n",
       "honest        0.965951\n",
       "horribly      0.966833\n",
       "illustrious   0.966942\n",
       "luxurious     0.968997\n",
       "rogue         0.972301\n",
       "hilarious     0.974727\n",
       "clever        0.974823\n",
       "expansive     0.975288\n",
       "pork          0.978309\n",
       "brutally      0.979251\n",
       "hopefully     0.988199"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_scores_df[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "922b65cc-6d8a-4e19-9f31-165e6617f46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_scores_df.plot(kind=\"hist\", bins=100, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c440dbcc-263a-46f5-b230-4d7dd6baee45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7d06ac5-ca54-4489-888b-8fc2bd4dc465",
   "metadata": {},
   "source": [
    "**TO-DO:**\n",
    "- save out support (number of occurences)\n",
    "- Top 3 / bottom 3 from each sentence rather than taking all scores\n",
    "- Median vs. mean?\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e7cce14f-e2ed-486f-b319-3ab5ef7bbf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(sc[\"the\"]).plot(kind=\"hist\", bins=100, figsize=(15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2b5283f8-05c6-4118-a897-d19494190af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.055765\n",
       "dtype: float64"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sc[\"the\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1bc00202-8fd1-4d5e-8870-482a4f978c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.036598\n",
       "dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sc[\"the\"]).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7bf29d-94bb-48f1-b08a-d5b703eb2211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1ee6069c-7fe2-4656-a455-532842039f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_collection = defaultdict(list)\n",
    "\n",
    "# cls_explainer = SequenceClassificationExplainer(model, tokenizer)\n",
    "word_attributions = cls_explainer(test3, index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "62fec46a-8e2c-4056-8ddc-c12bb56b8c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', 0.0)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_attributions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "db0b471a-9065-4074-a974-236a3a2f7cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, score in word_attributions:\n",
    "    test_collection[token].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9176a5-29fc-43fd-aad3-50ca5a35366a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4f7e6-d2c3-48f8-a981-f78a4054a9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b38176a7-cc50-4720-8510-bacdac4249a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_from_disk(CLS_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7d287027-ab79-47cb-90b2-d1abbe09cd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"following the end of kenneth kaunda's repressive dictatorship , chiluba won the country's multi-party presidential elections.\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].to_pandas()[\"text\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7dfa435e-0c1b-4886-bb6f-0856287b94fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"following the end of kenneth kaunda's presidency , chiluba won the country's multi-party presidential elections.\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].to_pandas()[\"text\"][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb461f0-41d2-4190-b7c2-2519c5d7dc21",
   "metadata": {},
   "source": [
    "**For Tomorrow:**\n",
    "\n",
    "- come up with high level talking points to describe word attribution (how it actually works)\n",
    "- lay out example of word attributions above\n",
    "- Then, figure out \"Style Token Masking\"\n",
    "    - What thresholds on word attribution scores?\n",
    "    - What threshold on number of words?\n",
    "    - Can I develop these based on looking at distributions from train set?\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0aae2-0999-438a-b47b-4e17f99b3dae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
