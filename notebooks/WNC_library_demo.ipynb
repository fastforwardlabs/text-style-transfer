{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# \"Getting Started\" with Text Style Transfer Library\n",
    "\n",
    "This notebook is intented to serve as a demonstration of how to use the main functionality developed during the _FF24: Text Style Transfer_ research cycle. In particular we walk through the high-level usage and explose the low-level, inner workings of the following classes: `SubjectivityNeutralizer`, `StyleIntensityClassifier`, and `ContentPreservationScorer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `SubjectivityNeutralizer` Walkthrough\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/tst_bart.png\"/></center>\n",
    "<br>\n",
    "\n",
    "The `SubjectivityNeutralizer` class consists of a sequence-to-sequence model wrapper around a HuggingFace pipeline that can be used to generate neutral-toned text provided subjective text as input. The class must be initialized with a HuggingFace model identifier for the weights of a fine-tuned BART model on the Wiki Neutrality Corpus (WNC). The `.transfer()` method takes a list of strings as input to return a corresponding list of strings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import SubjectivityNeutralizer\n",
    "\n",
    "# instantiate class with TST model path\n",
    "MODEL_PATH = \"/home/cdsw/models/bart-tst-full\"\n",
    "sn = SubjectivityNeutralizer(model_identifier=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir Alex Ferguson is one of the greatest football managers of all time.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate neutralized text conditioned on the subjective input text\n",
    "examples = [\"Sir Alex Ferguson is the greatest football manager of all time.\"]\n",
    "sn.transfer(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## `StyleIntensityClassifier` Walkthrough\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/style_transfer_intensity.png\"/></center>\n",
    "<br>\n",
    "\n",
    "Evaluating the quality of a text style transfer model is a difficult task as there is no \"standard\" set of evaluation practices or metric definitions. [Existing literature](https://arxiv.org/pdf/1904.02295.pdf) on the topic considers three main aspects of quality: style transfer intensity, content preservation, and fluency. The `StyleIntensityClassifer` that we have defined here draws inspiration from the mentioned paper and implements one way to measure the first of those aspects -- style transfer intensity (STI).\n",
    "\n",
    "The STI metric can be explained best by referencing the figure above:\n",
    "1. A fine-tuned text style transfer model (BART) is used to generate neutralized text ($X_{N}$) from a subjective input ($X_{S}$). This forms the pair of text that we will be calculating the style transfer intensity between.\n",
    "2. Both texts are passed through a fine-tuned, Transformer-based classification model (BERT) to produce a resulting style distribution for each text ($d_{S}$, $d_{N}$). The BERT model here has been trained/fine-tuned on the style classification task for which the style transfer model was also trained on. In this case, that means classifying a given piece of text as subjective or neutral.\n",
    "3. Earth movers distance (EMD) -- also known as Wasserstein distance -- is then calculated on the two distributions to produce a resulting style transfer intensity score. The EMD distance metric calculates the minimum \"cost\" to turn one distribution into the other. In this sense, we can interpret EMD on style class distributions to imply how intense the style transfer was between the two input texts.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import StyleIntensityClassifier\n",
    "\n",
    "# instantiate class with classifier model path\n",
    "MODEL_PATH = \"../models/TRIAL-J-shuffle-lr_3en06-epoch_15-wd_.1-bs_32/checkpoint-67466\"\n",
    "sc = StyleIntensityClassifier(model_identifier=MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there is a roadhouse, named \"spud\\'s roadhouse\", which sells fuel and general shop items and has accommodation.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. transfer style using seq2seq model\n",
    "x_s = [\n",
    "    \"\"\"there is an iconic roadhouse, named \"spud's roadhouse\", which sells fuel and general shop items , has great meals and has accommodation.\"\"\"\n",
    "]\n",
    "\n",
    "x_n = sn.transfer(x_s)\n",
    "x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'label': 'LABEL_0',\n",
       "  'score': 0.9891378283500671,\n",
       "  'distribution': [0.9891378283500671, 0.010862216353416443]},\n",
       " {'label': 'LABEL_1',\n",
       "  'score': 0.9893038272857666,\n",
       "  'distribution': [0.010696199722588062, 0.9893038272857666]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. obtain style distributions using BERT classifier\n",
    "d_s, d_n = sc.score(x_s + x_n)\n",
    "d_s, d_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9784"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. calculate EMD between d_s and d_n\n",
    "sc.calculate_emd(\n",
    "    input_dist=d_s[\"distribution\"], output_dist=d_n[\"distribution\"], target_class_idx=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### High-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9784]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.calculate_transfer_intensity(input_text=x_s, output_text=x_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ContentPreservationScorer` Walkthrough\n",
    "\n",
    "<br>\n",
    "<center><img src=\"./images/content_preservation_score.png\"/></center>\n",
    "<br>\n",
    "\n",
    "Similar to our STI metric, the Content Preservation Score (CPS) metric also draws inspiration from the previously mentioned paper, and aims to quantify the similarity in content (i.e. style-independent semantic meaning) between the input and the output texts. The metric is depicted in the figure above:\n",
    "\n",
    "1. A fine-tuned text style transfer model (BART) is used to generate neutralized text ($X_{N}$) from a subjective input ($X_{S}$). This forms the pair of text that we will be calculating the style transfer intensity between.\n",
    "2. Style tokens are masked inline in both texts to produce versions of the text that contain only content-related tokens. Style tokens are determined by calculating word attributions for each text on a per-sentence basis using integrated gradients from the trained BERT classification model. Essentially, this method produces per-token feature importances, and tokens that have a high attribution score (i.e. are important in making a style classification) are deemed as style-related tokens.\n",
    "3. The style-masked texts are then passed through a generic, pre-trained (but not fine-tuned) SentenceBERT model to produce sentence embeddings for each text ($e_{S}$, $e_{N}$).\n",
    "4. We calculate cosine similarity between these content-only embedding reprentations. Since the style-related tokens have been removed from the text, high cosine similarity between these embeddings indicates a high level of content preservation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.inference import ContentPreservationScorer\n",
    "\n",
    "# instantiate class with classifier model path\n",
    "SBERT_MODEL_PATH = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CLS_MODEL_PATH = (\n",
    "    \"../models/TRIAL-J-shuffle-lr_3en06-epoch_15-wd_.1-bs_32/checkpoint-67466\"\n",
    ")\n",
    "cps = ContentPreservationScorer(\n",
    "    sbert_model_identifier=SBERT_MODEL_PATH, cls_model_identifier=CLS_MODEL_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-by-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alexithymia is claimed to affect 10% of the overall population.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. transfer style using seq2seq model\n",
    "x_s = [\"alexithymia is thought to affect 10% of the overall population.\"]\n",
    "\n",
    "x_n = sn.transfer(x_s)\n",
    "x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('alexithymia is [PAD] to affect 10 % of the overall population.',\n",
       " 'alexithymia is [PAD] to affect 10 % of the overall population.')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. mask out style-related tokens from both texts\n",
    "#\n",
    "# NOTE: \"threshold\" indicates the cumulative percentage of style-attributed tokens\n",
    "# that should be masked out. \"mask_type\" specifies if we should replace style tokens\n",
    "# with a \"[PAD]\" token or remove it outright\n",
    "x_s_masked = cps.mask_style_tokens(text=x_s[0], threshold=0.1, mask_type=\"pad\")\n",
    "x_n_masked = cps.mask_style_tokens(text=x_n[0], threshold=0.1, mask_type=\"pad\")\n",
    "\n",
    "x_s_masked, x_n_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>score</th>\n",
       "      <th>abs_norm</th>\n",
       "      <th>cumulative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>claimed</td>\n",
       "      <td>-0.932072</td>\n",
       "      <td>0.462688</td>\n",
       "      <td>0.462688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>affect</td>\n",
       "      <td>0.243048</td>\n",
       "      <td>0.120651</td>\n",
       "      <td>0.583339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.125258</td>\n",
       "      <td>0.062179</td>\n",
       "      <td>0.645517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>overall</td>\n",
       "      <td>-0.116233</td>\n",
       "      <td>0.057699</td>\n",
       "      <td>0.703217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>is</td>\n",
       "      <td>-0.103953</td>\n",
       "      <td>0.051603</td>\n",
       "      <td>0.754820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>population</td>\n",
       "      <td>-0.088188</td>\n",
       "      <td>0.043777</td>\n",
       "      <td>0.798597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>##ia</td>\n",
       "      <td>-0.088092</td>\n",
       "      <td>0.043730</td>\n",
       "      <td>0.842326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>to</td>\n",
       "      <td>-0.071400</td>\n",
       "      <td>0.035444</td>\n",
       "      <td>0.877770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>the</td>\n",
       "      <td>-0.069723</td>\n",
       "      <td>0.034611</td>\n",
       "      <td>0.912381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>##ith</td>\n",
       "      <td>-0.047825</td>\n",
       "      <td>0.023741</td>\n",
       "      <td>0.936121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alex</td>\n",
       "      <td>0.046381</td>\n",
       "      <td>0.023024</td>\n",
       "      <td>0.959145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>##ym</td>\n",
       "      <td>-0.032494</td>\n",
       "      <td>0.016130</td>\n",
       "      <td>0.975275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>-0.031798</td>\n",
       "      <td>0.015785</td>\n",
       "      <td>0.991060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>%</td>\n",
       "      <td>-0.009316</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.995685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>of</td>\n",
       "      <td>-0.008693</td>\n",
       "      <td>0.004315</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[SEP]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token     score  abs_norm  cumulative\n",
       "6      claimed -0.932072  0.462688    0.462688\n",
       "8       affect  0.243048  0.120651    0.583339\n",
       "9           10 -0.125258  0.062179    0.645517\n",
       "13     overall -0.116233  0.057699    0.703217\n",
       "5           is -0.103953  0.051603    0.754820\n",
       "14  population -0.088188  0.043777    0.798597\n",
       "4         ##ia -0.088092  0.043730    0.842326\n",
       "7           to -0.071400  0.035444    0.877770\n",
       "12         the -0.069723  0.034611    0.912381\n",
       "2        ##ith -0.047825  0.023741    0.936121\n",
       "1         alex  0.046381  0.023024    0.959145\n",
       "3         ##ym -0.032494  0.016130    0.975275\n",
       "15           . -0.031798  0.015785    0.991060\n",
       "10           % -0.009316  0.004625    0.995685\n",
       "11          of -0.008693  0.004315    1.000000\n",
       "0        [CLS]  0.000000  0.000000    1.000000\n",
       "16       [SEP]  0.000000  0.000000    1.000000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: In x_n from the example above, we mask out the \"claimed\" token because\n",
    "# as we see from below, it is attributed to 46.2% of style for this sentence\n",
    "# and we set a threhold of 0.1, so just this one token is selected for masking\n",
    "cps.calculate_feature_attribution_scores(text=x_n[0], as_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 384]), torch.Size([1, 384]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. get sentence embeddings from SBERT for each masked text\n",
    "\n",
    "e_s = cps.compute_sentence_embeddings(input_text=x_s_masked)\n",
    "e_n = cps.compute_sentence_embeddings(input_text=x_n_masked)\n",
    "\n",
    "e_s.shape, e_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. calculate cosine similarity between style-removed embedding representations\n",
    "cps.cosine_similarity(e_s, e_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scores': [1.0],\n",
       " 'masked_input_text': ['alexithymia is [PAD] to affect 10 % of the overall population.'],\n",
       " 'masked_output_text': ['alexithymia is [PAD] to affect 10 % of the overall population.']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cps.calculate_content_preservation_score(\n",
    "    input_text=x_s, output_text=x_n, threshold=0.1, mask_type=\"pad\", return_all=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
