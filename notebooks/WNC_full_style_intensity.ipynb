{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a5873d-7e70-45c4-af1f-cca45462f040",
   "metadata": {},
   "source": [
    "# Style Transfer Intensity (STI)\n",
    "\n",
    "Style Transfer Strength is often evaluated by training a classifier on the labeled dataset and measuring the number of outputes classified as having the target style. \n",
    "\n",
    "[This paper](https://arxiv.org/pdf/1904.02295.pdf) proposes an alternative method.\n",
    "\n",
    "Rather than count how many output texts achieve a target style, we can capture more nuanced differences between the style distributions of x and x', using Earth Mover’s Distance.EMD is the minimum “cost” to turn one distribution into the other, or how “intense” the transfer is. Distributions can have any number of values (styles), so EMD handles binary and non-binary datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba1c4a-1442-47cf-a050-28c40e458163",
   "metadata": {},
   "source": [
    "## Prepare WNC for style classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731d9d4c-dc0c-4e02-b798-c3fc956363f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    load_from_disk,\n",
    "    load_metric,\n",
    "    Dataset,\n",
    "    Features,\n",
    "    Value,\n",
    "    ClassLabel,\n",
    "    DatasetDict,\n",
    ")=\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f92c9666-64cd-45b4-813f-cc0595bf5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classification_dataset(path: str) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Formats the translation-task version of WNC as a classification dataset.\n",
    "\n",
    "    Dataset splits remain the same, but the number of records in each split are doubled\n",
    "    as we create an individual record for both the \"source_text\" and \"target_text\" fields.\n",
    "    In this way, \"source_text\" is assigned a label of \"subjective\" and \"target_text\" is assigned\n",
    "    a label of \"neutral\". Records are randomly shuffled within each split.\n",
    "\n",
    "    Args:\n",
    "        path (str): path to HuggingFace dataset\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict\n",
    "\n",
    "    \"\"\"\n",
    "    datasets = load_from_disk(path)\n",
    "    dataset_dict = defaultdict(dict)\n",
    "\n",
    "    SPLITS = [\"train\", \"test\", \"validation\"]\n",
    "    LABEL_MAPPING = {\"source_text\": \"subjective\", \"target_text\": \"neutral\"}\n",
    "    FEATURES = Features(\n",
    "        {\n",
    "            \"text\": Value(\"string\"),\n",
    "            \"label\": ClassLabel(num_classes=2, names=[\"subjective\", \"neutral\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for split in SPLITS:\n",
    "        df = datasets[split].to_pandas()\n",
    "        split_dict = defaultdict(list)\n",
    "\n",
    "        for column, label in LABEL_MAPPING.items():\n",
    "            split_dict[\"text\"].extend(df[column].tolist())\n",
    "            split_dict[\"label\"].extend([label] * len(df))\n",
    "\n",
    "        # reorder records so subjective/neutral pairs alternate in sequence\n",
    "        temp_df = pd.DataFrame(split_dict)\n",
    "        dfs = np.split(temp_df, indices_or_sections=2, axis=0)\n",
    "        dfs = [df.reset_index(drop=True) for df in dfs]\n",
    "        temp_df = pd.concat(dfs).sort_index(kind=\"merge\").reset_index(drop=True)\n",
    "\n",
    "        dataset_dict[split] = Dataset.from_dict(\n",
    "            temp_df.to_dict(orient=\"list\"), features=FEATURES\n",
    "        )\n",
    "\n",
    "    return DatasetDict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e7a4d33-c201-46a9-b48b-e85c7667465e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASETS_PATH = \"/home/cdsw/data/processed/WNC_full\"\n",
    "wnc_classification = build_classification_dataset(DATASETS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742aa0b9-635f-433f-9f6b-250bddfec7d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 308394\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 17154\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 17214\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e67d3800-35c0-44e3-80d1-c5e4c2d74384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': [\"following the end of kenneth kaunda's repressive dictatorship , chiluba won the country's multi-party presidential elections.\",\n",
       "  \"following the end of kenneth kaunda's presidency , chiluba won the country's multi-party presidential elections.\",\n",
       "  'a brilliant quarterback with the university of illinois, haller was signed by the giants as an amateur free agent in 1958. he made his debut on april 11, 1961 as a platoon catcher.',\n",
       "  'a quarterback with the university of illinois, haller was signed by the giants as an amateur free agent in 1958. he made his debut on april 11, 1961 as a platoon catcher.',\n",
       "  'traitor to his people adam yahiye gadahn (born september 1, 1978) is an american-born man who is suspected of being a member of the al qaeda organization.',\n",
       "  'adam yahiye gadahn (born september 1, 1978) is an american-born man who is suspected of being a member of the al qaeda organization.',\n",
       "  'a funny thing happened on the way to the moon is a 2001 documentary written, produced, and directed by nashville, tennessee-based filmmaker and investigative journalist bart winfield sibrel, a critic of the united states space program and proponent of the unsubstantiated theory that the six apollo lunar landing missions between 1969 and 1972 were hoaxes perpetrated by the us government.',\n",
       "  'a funny thing happened on the way to the moon is a 2001 documentary written, produced, and directed by nashville, tennessee-based filmmaker and investigative journalist bart winfield sibrel, a critic of the united states space program and proponent of the theory that the six apollo lunar landing missions between 1969 and 1972 were hoaxes perpetrated by the us government.',\n",
       "  'the redcliffe forward, originally from roma, queensland, spent four months in france with the tonneins club in 1982-83 before using the state of origin stage to force his way into the australian side against new zealand in 1983. a rugged, no-nonsense back-rower, fullerton smith played in the last two tests against great britain in 1984 before joining english club leeds in the 1984-85 off-season.',\n",
       "  'the redcliffe forward, originally from roma, queensland, spent four months in france with the tonneins club in 1982-83 before using the state of origin stage to force his way into the australian side against new zealand in 1983. fullerton smith played in the last two tests against great britain in 1984 before joining english club leeds in the 1984-85 off-season.'],\n",
       " 'label': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc_classification[\"train\"][2:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b75e9d0-f14a-4434-9f53-71a0f8c6d0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subjective'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc_classification[\"train\"].features[\"label\"].int2str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5e4d6cb-0565-4576-ab23-30d83a06cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset\n",
    "CLS_DATASET_PATH = \"/home/cdsw/data/processed/WNC_cls_full\"\n",
    "os.makedirs(CLS_DATASET_PATH)\n",
    "wnc_classification.save_to_disk(CLS_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7a58e4-984c-4af2-909f-2ffea7254331",
   "metadata": {},
   "source": [
    "### Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "090047ab-0ba6-4c43-b3cb-26f24c7b1a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107c88c488ce489eac628bf13562fb30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1657932aeab64a6fb4bc1ac7ebc05a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be66dca8f48a460e92d0b5e51a94f8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_wnc_classification = DatasetDict(\n",
    "    {\n",
    "        \"train\": wnc_classification[\"train\"].select(range(1000)),\n",
    "        \"test\": wnc_classification[\"test\"].select(range(1000)),\n",
    "        \"validation\": wnc_classification[\"validation\"].select(range(1000)),\n",
    "    }\n",
    ")\n",
    "\n",
    "TEST_CLS_DATASET_PATH = \"/home/cdsw/data/processed/WNC_full_cls_TEST\"\n",
    "os.makedirs(TEST_CLS_DATASET_PATH)\n",
    "test_wnc_classification.save_to_disk(TEST_CLS_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72556471-c4ed-4246-adab-bb20a4f1b62a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_wnc_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f536ac-daf5-4473-8a11-c85c2191e6a5",
   "metadata": {},
   "source": [
    "## Train a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "903cdbee-1d11-447a-9280-c4b1480f0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_DATASET_PATH = \"/home/cdsw/data/processed/WNC_cls_full\"\n",
    "wnc_full_cls = load_from_disk(CLS_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad2a170-cd09-4b86-8501-6aec3f1ceaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 308394\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 17154\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 17214\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc_full_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a40a6fce-fc21-46ae-b86d-3626bcd54d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>while for long nearly only women where shown a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>increased tolerance, more tempered censorship,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>following the end of kenneth kaunda's repressi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>following the end of kenneth kaunda's presiden...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a brilliant quarterback with the university of...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308389</th>\n",
       "      <td>regardless of how a received message is format...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308390</th>\n",
       "      <td>in peloponnesos, at any rate, the revolution h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308391</th>\n",
       "      <td>in peloponnesos, at any rate, the revolution h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308392</th>\n",
       "      <td>communism disregarded and hated man : roy in h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308393</th>\n",
       "      <td>roy in his philosophy devised means to ensure ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>308394 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       while for long nearly only women where shown a...      0\n",
       "1       increased tolerance, more tempered censorship,...      1\n",
       "2       following the end of kenneth kaunda's repressi...      0\n",
       "3       following the end of kenneth kaunda's presiden...      1\n",
       "4       a brilliant quarterback with the university of...      0\n",
       "...                                                   ...    ...\n",
       "308389  regardless of how a received message is format...      1\n",
       "308390  in peloponnesos, at any rate, the revolution h...      0\n",
       "308391  in peloponnesos, at any rate, the revolution h...      1\n",
       "308392  communism disregarded and hated man : roy in h...      0\n",
       "308393  roy in his philosophy devised means to ensure ...      1\n",
       "\n",
       "[308394 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc_full_cls[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d4700b4-674a-4f77-9b2e-e3e5fcf72cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from transformers.integrations import MLflowCallback\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0d33d0f-88f0-4165-bf29-81fb847e0e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cdsw/data/processed/WNC_cls_full/train/cache-0a1520a3bb15d3a6.arrow\n",
      "Loading cached processed dataset at /home/cdsw/data/processed/WNC_cls_full/test/cache-7534370130f2438d.arrow\n",
      "Loading cached processed dataset at /home/cdsw/data/processed/WNC_cls_full/validation/cache-a3249b64e9d2c2a2.arrow\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = wnc_full_cls.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8c51b4-e653-4dfe-992b-cb2345a07fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 308394\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 17154\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 17214\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b35b4152-9949-44cb-9973-749d8a808ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-cls-full-nbtest\"\n",
    "MODEL_DIR = \"/home/cdsw/models\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(MODEL_DIR, MODEL_NAME),\n",
    "    learning_rate=5e-05,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    logging_dir=os.path.join(MODEL_DIR, \"logs\", MODEL_NAME),\n",
    "    logging_steps=50,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_total_limit=5,\n",
    "    save_steps=50,\n",
    "    # metric_for_best_model=\"f1\",\n",
    "    # greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9bb2536-5611-49b4-890e-d9280aa91d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a5f6eb9-8b96-4db0-98e4-5089c2e8ca1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148dd7fe-cfe7-41c6-9775-360983186253",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.remove_callback(MLflowCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "929f0f63-a21e-4c2e-9042-e8674b65e04c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/cdsw/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 308394\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 96375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='96375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  151/96375 01:25 < 15:25:17, 1.73 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.706700</td>\n",
       "      <td>0.693016</td>\n",
       "      <td>0.501394</td>\n",
       "      <td>0.666330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.689600</td>\n",
       "      <td>0.692646</td>\n",
       "      <td>0.504880</td>\n",
       "      <td>0.050574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='366' max='1076' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 366/1076 00:09 < 00:17, 40.42 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/checkpoint-50\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-50/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-50/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-50/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-50/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/checkpoint-100\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-100/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-100/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1473\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1475\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1476\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1600\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1602\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1603\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2256\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2257\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2258\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2430\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2431\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   2639\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2640\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2641\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2642\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1545\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1546\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    994\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         )\n\u001b[0;32m--> 996\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    997\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 )\n\u001b[1;32m    584\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    586\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    473\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     ):\n\u001b[0;32m--> 402\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1097\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1098\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62617b-9329-4311-9683-8b27d4303542",
   "metadata": {},
   "source": [
    "## Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5fe638-ce09-4747-92d3-35d14ec9e2be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_disk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-efa822c06e82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mCLS_DATASET_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/cdsw/data/processed/WNC_cls_full\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwnc_full_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLS_DATASET_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_from_disk' is not defined"
     ]
    }
   ],
   "source": [
    "CLS_DATASET_PATH = \"/home/cdsw/data/processed/WNC_cls_full\"\n",
    "wnc_full_cls = load_from_disk(CLS_DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4658f-8bca-4ad6-a316-de767b9a0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnc_full_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bf6c21-e6c4-4be9-ad89-433a9ea21919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from torch.utils.data import SequentialSampler, BatchSampler, DataLoader\n",
    "from transformers.integrations import MLflowCallback\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebab1753-905b-4bb0-be21-70626b287049",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wnc_full_cls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9a3ae58dfd15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtokenized_datasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwnc_full_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wnc_full_cls' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = wnc_full_cls.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d2ba38f-c6ab-4c57-b7e2-6032abe4274a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 308394\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 17154\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 17214\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9225280-95c6-494f-89c9-4fa6be63c912",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/cdsw/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/cdsw/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834282c5-e503-439e-a47b-691b086bedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "\n",
    "    accuracy_metric = load_metric(\"accuracy\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return accuracy_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d2faf441-a7c2-4a0c-acb5-9f98b67ca49f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"bert-cls-full-blah\"\n",
    "MODEL_DIR = \"/home/cdsw/models\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(MODEL_DIR, MODEL_NAME),\n",
    "    learning_rate=3e-05,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    # num_train_epochs=2,\n",
    "    logging_dir=os.path.join(MODEL_DIR, \"logs\", MODEL_NAME),\n",
    "    logging_steps=500,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_total_limit=1,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    # metric_for_best_model=metric_name,\n",
    "    # metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c33bab1-1c88-40de-bf42-679f15881beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    A custom Trainer that overwrites and subclasses the `get_train_dataloader()` method.\n",
    "\n",
    "    This customization allows us to introduce a flag that disables shuffling on the DataLoader. When\n",
    "    `shuffle_train` flag is True, a RandomSampler is used via `self._get_train_sampler`. When set to False,\n",
    "    a SequentialSampler is utilized in the dataloader.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shuffle_train, *args, **kwargs):\n",
    "        self.shuffle_train = shuffle_train\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def seed_worker(self, _):\n",
    "        \"\"\"\n",
    "        Helper function to set worker seed during Dataloader initialization.\n",
    "        \"\"\"\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        set_seed(worker_seed)\n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        if isinstance(train_dataset, Dataset):\n",
    "            train_dataset = self._remove_unused_columns(\n",
    "                train_dataset, description=\"training\"\n",
    "            )\n",
    "            print(train_dataset)\n",
    "        else:\n",
    "            data_collator = self._get_collator_with_removed_columns(\n",
    "                data_collator, description=\"training\"\n",
    "            )\n",
    "\n",
    "        if self.shuffle_train:\n",
    "            train_sampler = self._get_train_sampler()\n",
    "        else:\n",
    "            train_sampler = SequentialSampler(self.train_dataset)\n",
    "\n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "            worker_init_fn=self.seed_worker,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ade2687-d8a1-4bd0-876c-88a543f4f94a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].shard(index=1, num_shards=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    # shuffle_train=True,\n",
    "    # model_init=model_init,\n",
    "    model,\n",
    "    args=training_args,\n",
    "    # train_dataset=train_dataset,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.remove_callback(MLflowCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd364073-533c-4391-8a58-25baf61cf0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `Trainer()` not found.\n"
     ]
    }
   ],
   "source": [
    "?Trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3ba40d9-50e2-4a6f-ae55-ffc735813ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/cdsw/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 308394\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 57825\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2034' max='57825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2034/57825 04:35 < 2:06:00, 7.38 it/s, Epoch 0.11/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.705300</td>\n",
       "      <td>0.696181</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.703200</td>\n",
       "      <td>0.698130</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-blah/checkpoint-1000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-blah/checkpoint-1000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-blah/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-blah/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-blah/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-blah/checkpoint-2000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-blah/checkpoint-2000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-blah/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-blah/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-blah/checkpoint-2000/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 if (\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b3fe6e8-fbb0-4b33-8253-04670f90e812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/cdsw/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/cdsw/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 30840\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2892\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2892' max='2892' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2892/2892 10:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>5.994174</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.370475</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.492270</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/checkpoint-500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/checkpoint-1000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-1000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/checkpoint-1500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-1500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/checkpoint-2000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-2000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/checkpoint-2500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-2500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2892, training_loss=0.0007404177269707067, metrics={'train_runtime': 633.3754, 'train_samples_per_second': 146.075, 'train_steps_per_second': 4.566, 'total_flos': 3896571952125600.0, 'train_loss': 0.0007404177269707067, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67c02a85-4558-4ae6-9bdf-e46f212c3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hp_space(trial):\n",
    "    # return {\n",
    "    #     \"learning_rate\": trial.suggest_float(\"learning_rate\", 3e-06, 3e-04, log=True),\n",
    "    #     \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.000001, 0.01, log=True),\n",
    "    #     # \"per_device_train_batch_size\": trial.suggest_categorical(\n",
    "    #     #     \"per_device_train_batch_size\", [8, 16]\n",
    "    #     # ),\n",
    "    #     # \"shuffle_train\": trial.suggest_categorical(\"shuffle_train\", [True, False]),\n",
    "    # }\n",
    "\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 3, 8),\n",
    "        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\n",
    "            \"per_device_train_batch_size\", [8, 16, 32]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def my_objective(metrics):\n",
    "    return metrics[\"eval_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e650cd-59ad-4b33-8d3d-6af5af17ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hp_space_ray(trial):\n",
    "    from ray import tune\n",
    "\n",
    "    return {\n",
    "        \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "        \"num_train_epochs\": tune.choice(range(1, 6)),\n",
    "        \"seed\": tune.choice(range(1, 41)),\n",
    "        \"per_device_train_batch_size\": tune.choice([4, 8, 16, 32, 64]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b90571a9-6a26-4ce5-a433-788777b65edd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-17 16:37:05,485]\u001b[0m A new study created in memory with name: no-name-65ef4c6f-12a4-45c3-b09e-f75d6f6198be\u001b[0m\n",
      "Trial:\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/cdsw/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/cdsw/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/cdsw/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 30840\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3856\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3856' max='3856' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3856/3856 14:11, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>5.116300</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.591533</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.830876</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.919676</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-1000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-1000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-1500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-1500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-2000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-2000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-2500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-2500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-2500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-3000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-3000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-3500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-3500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-0/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2022-05-17 16:51:20,340]\u001b[0m Trial 0 finished with value: 0.5 and parameters: {'learning_rate': 1.5935194039442208e-05, 'num_train_epochs': 4, 'seed': 22, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 0.5.\u001b[0m\n",
      "Trial:\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/cdsw/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/cdsw/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/cdsw/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 30840\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15420\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12129' max='15420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12129/15420 17:51 < 04:50, 11.32 it/s, Epoch 3.15/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.396152</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.223080</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.856434</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-1000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-1000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-1500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-1500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-2000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-2000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-2500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-2500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-3000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-3000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-3500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-3500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-3500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-4000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-4000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-4500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-4500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-5000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-5000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-5500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-5500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-6000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-6000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-6500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-6500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-7000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-7000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-7500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-7500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-7500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-8000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-8000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-8000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-8500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-8500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-8500/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-9000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-9000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-9000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-9500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-9500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-9500/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-10000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-10000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-10000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-10500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-10500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-10500/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-11000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-11000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-11000/special_tokens_map.json\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-11500\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-11500/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-11500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 17214\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-12000\n",
      "Configuration saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-12000/config.json\n",
      "Model weights saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in /home/cdsw/models/bert-cls-full-nbtest/run-1/checkpoint-12000/special_tokens_map.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-13727844a937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m best_run = trainer.hyperparameter_search(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mhp_space\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_hp_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcompute_objective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_objective\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"maximize\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"optuna\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mhyperparameter_search\u001b[0;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[1;32m   1889\u001b[0m             \u001b[0mHPSearchBackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWANDB\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_hp_search_wandb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1890\u001b[0m         }\n\u001b[0;32m-> 1891\u001b[0;31m         \u001b[0mbest_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1893\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhp_search_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/integrations.py\u001b[0m in \u001b[0;36mrun_hp_search_optuna\u001b[0;34m(trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mn_jobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"n_jobs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_objective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBestRun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    398\u001b[0m             )\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/integrations.py\u001b[0m in \u001b[0;36m_objective\u001b[0;34m(trial, checkpoint_dir)\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;31m# If there hasn't been any evaluation during the training loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 if (\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    hp_space=my_hp_space,\n",
    "    compute_objective=my_objective,\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    n_trials=4,\n",
    "    # resources_per_trial={\"cpu\": 1, \"gpu\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e75eba2-6833-49b4-9fc9-0b86af6e58b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='1', objective=0.5005228302544441, hyperparameters={'learning_rate': 1.2819901548249134e-06, 'num_train_epochs': 2, 'seed': 38, 'per_device_train_batch_size': 64})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4556d-2353-471e-b50b-295370303826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c6aae-5021-48f3-a888-ddd5c904710e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89de0ea7-ff18-4d52-89a2-10b355e742c9",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36d3346-0a07-4314-9240-fdf57d491c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [63/63 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7722688-868d-4e76-8349-ad5e8db0b031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3496061, -0.6291089],\n",
       "       [ 1.2544787, -2.2091932],\n",
       "       [ 1.4202304, -2.272884 ],\n",
       "       ...,\n",
       "       [ 1.9655778, -2.7637262],\n",
       "       [-2.3133612,  2.5391092],\n",
       "       [ 1.3082958, -2.4287748]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e975bb74-3a70-4d08-b276-a404f08bed3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74992d71-9d07-4c34-a84d-61e94cbdf70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3cb7665-03e8-49e0-9e9a-b3bf77b0a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fc6ba2c-e9f8-4264-8cf0-8da90e5bc6fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.626, 'f1': 0.625250501002004}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d6ffadc-7ee3-4103-a597-f2a1886ce067",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-fed3befee4fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-38a3d51cb394>\u001b[0m in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_preds)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrpc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "compute_metrics(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7619862f-a8a9-4fd2-a379-6f7f5e106773",
   "metadata": {},
   "source": [
    "#### manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cd8e324-7989-48a1-b554-2f34cf669900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0258575422b4e82a822dd705196c189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "f1_metric = load_metric(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "033ded69-44c1-424c-b3d7-d9d6e3430ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.626}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1d58085-bbe9-4f1e-a891-6758159c03d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.625250501002004}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.compute(predictions=preds, references=predictions.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0d107-6093-4217-8020-cb262716d04a",
   "metadata": {},
   "source": [
    "## Test custom DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c67edd1f-8328-4839-9cf0-af45ae8d655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74db7021-5fdc-4042-82d1-8934df7012f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SequentialSampler, BatchSampler, DataLoader\n",
    "from transformers import set_seed\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \"\"\"\n",
    "    A custom Trainer that overwrites and subclasses the `get_train_dataloader()` method.\n",
    "\n",
    "    This customization allows us to introduce a flag that disables shuffling on the DataLoader. When\n",
    "    `shuffle_train` flag is True, a RandomSampler is used via `self._get_train_sampler`. When set to False,\n",
    "    a BatchSampler(SequentialSampler()) is utilized in the dataloader.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shuffle_train, *args, **kwargs):\n",
    "        self.shuffle_train = shuffle_train\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def seed_worker(self, _):\n",
    "        \"\"\"\n",
    "        Helper function to set worker seed during Dataloader initialization.\n",
    "        \"\"\"\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        set_seed(worker_seed)\n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        if isinstance(train_dataset, Dataset):\n",
    "            print(\"Removed unused columns\")\n",
    "            train_dataset = self._remove_unused_columns(\n",
    "                train_dataset, description=\"training\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"New data collator\")\n",
    "            data_collator = self._get_collator_with_removed_columns(\n",
    "                data_collator, description=\"training\"\n",
    "            )\n",
    "            print(data_collator)\n",
    "\n",
    "        if self.shuffle_train:\n",
    "            train_sampler = self._get_train_sampler()\n",
    "        else:\n",
    "            train_sampler = SequentialSampler(self.train_dataset)\n",
    "\n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "            worker_init_fn=self.seed_worker,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dead2f44-ccfd-4f39-93d5-650f58d2d4c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6bfd6129ed6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentence2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_datasets' is not defined"
     ]
    }
   ],
   "source": [
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"text\", \"label\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3a684e7-8555-484d-9818-94610f6cbe77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 62]),\n",
       " 'token_type_ids': torch.Size([8, 62]),\n",
       " 'attention_mask': torch.Size([8, 62])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ddcb3f5-305e-4666-afae-41486866e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list(\n",
    "    BatchSampler(\n",
    "        SequentialSampler(wnc_full_cls[\"train\"]), batch_size=3, drop_last=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bbcbadd8-f79f-47b7-b29c-6b466bdb9592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc_full_cls[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724f7e7-bada-45fe-a5eb-2c0ed435942c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7eebd1-8c78-41dd-abfd-48b4b9348651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "598da1cf-b8da-4254-83f4-1f74802aa32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/TESTING123\",\n",
    "    per_device_train_batch_size=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1ab3e4-cb9d-467e-ac9f-ba7e31715cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/cdsw/data/processed/WNC_full_cls_TEST/train/cache-11a8b3faaf6e3c0a.arrow\n",
      "Loading cached processed dataset at /home/cdsw/data/processed/WNC_full_cls_TEST/test/cache-5c62f87956a8bec2.arrow\n",
      "Loading cached processed dataset at /home/cdsw/data/processed/WNC_full_cls_TEST/validation/cache-8db993456d6202ea.arrow\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load base-model and tokenizer\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = wnc_full_cls.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ad2fbb-54e9-4c6b-88c1-b55667c04d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c57f4b33-6c98-4306-90e9-e5c3dce08359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "\n",
    "    accuracy_metric = load_metric(\"accuracy\")\n",
    "    f1_metric = load_metric(\"f1\")\n",
    "\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=predictions, references=labels),\n",
    "        \"f1\": f1_metric.compute(predictions=predictions, references=labels),\n",
    "    }\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    shuffle_train=False,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.remove_callback(MLflowCallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "794504e0-0442-4b89-9b5e-6e0cbd9e1edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/cdsw/.local/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed unused columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 00:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=0.5969503493536086, metrics={'train_runtime': 19.458, 'train_samples_per_second': 154.178, 'train_steps_per_second': 9.713, 'total_flos': 115834642122240.0, 'train_loss': 0.5969503493536086, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af6d9d73-199d-4fce-a9da-7d25e5987387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed unused columns\n"
     ]
    }
   ],
   "source": [
    "tdl = trainer.get_train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "474a49df-255e-4a5d-9c5f-125ce1bfb811",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdl_iter = iter(tdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5bf2ede-2b3a-4b7e-b57c-40e5f78adaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = next(tdl_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "98db6f77-e240-4ee4-ba34-95d483f36f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "second = next(tdl_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a9cd55fb-cd82-4048-a360-dc46e82af19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "third = next(tdl_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7d92bfe7-8c46-4a83-8e46-9271a64a2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "67e13816-6505-4906-8992-6c03f1180c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] two spanish ships with reinforcements, however, under the notorious hernando de soto, soon arrived by sea ( with at least a hundred volunteers ), and on these ships the spaniards, bound for more fruitful conquests on the peruvian mainland, embarked without incident and sailed back towards tumbes, arriving there on may 16, 1532. [SEP]'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(third[\"input_ids\"].tolist()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d6d66712-a64b-481f-a6e9-51bc866ab265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two spanish ships with reinforcements, however, under the notorious hernando de soto, soon arrived by sea (with at least a hundred volunteers), and on these ships the spaniards, bound for more fruitful conquests on the peruvian mainland, embarked without incident and sailed back towards tumbes, arriving there on may 16, 1532.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][\"text\"][6 + i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9e214-d5a0-48e2-90bd-d75004e52656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef045229-7a2e-425a-b6d2-be76b34c8aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "239d703d-5aea-4b1f-98f5-19d1ac403937",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] against this background his murder became a political cause celebre and cpi ( m )'s political opponents turned the needle of suspicion on them. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] against this background his murder became a political cause celebre and cpi ( m )'s political opponents turned the needle of suspicion on them. [SEP]\n",
      "\n",
      "----------------------------\n",
      "[CLS] ddd is licensed under the gnu general public license and is open source. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] ddd is licensed under the gnu general public license and is open source. [SEP]\n",
      "\n",
      "----------------------------\n",
      "[CLS] super bowl xliv was an american football game between the american football conference ( afc ) champion indianapolis colts and the national football conference ( nfc ) champion new orleans saints to decide the national football league ( nfl ) champion for the 2009 season, and peyton manning cemented his reputation of being un - clutch. [SEP]\n",
      "[CLS] super bowl xliv was an american football game between the american football conference ( afc ) champion indianapolis colts and the national football conference ( nfc ) champion new orleans saints to decide the national football league ( nfl ) champion for the 2009 season, and peyton manning cemented his reputation of being un - clutch. [SEP]\n",
      "\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(tokenizer.decode(first[\"input_ids\"].tolist()[i]))\n",
    "    print(tokenizer.decode(tokenized_datasets[\"train\"][\"input_ids\"][i]))\n",
    "    print()\n",
    "    print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e88ddfde-174e-47f0-80e1-8f12735b1b22",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"against this background his murder became a political cause celebre and cpi(m)'s political opponents turned the needle of suspicion on them .\",\n",
       " 'label': 0,\n",
       " 'input_ids': [101,\n",
       "  2114,\n",
       "  2023,\n",
       "  4281,\n",
       "  2010,\n",
       "  4028,\n",
       "  2150,\n",
       "  1037,\n",
       "  2576,\n",
       "  3426,\n",
       "  8292,\n",
       "  2571,\n",
       "  13578,\n",
       "  1998,\n",
       "  28780,\n",
       "  1006,\n",
       "  1049,\n",
       "  1007,\n",
       "  1005,\n",
       "  1055,\n",
       "  2576,\n",
       "  7892,\n",
       "  2357,\n",
       "  1996,\n",
       "  12201,\n",
       "  1997,\n",
       "  10928,\n",
       "  2006,\n",
       "  2068,\n",
       "  1012,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e96543d-c5fd-439b-839b-fd67a7473949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc478df-7e3d-41b0-8122-5f52c5deb2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8107a8ca-08a9-4cbd-8abb-2d0e8cff166a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1999,\n",
       " 3172,\n",
       " 1010,\n",
       " 1996,\n",
       " 2316,\n",
       " 2207,\n",
       " 2046,\n",
       " 1996,\n",
       " 4242,\n",
       " 1010,\n",
       " 1037,\n",
       " 9019,\n",
       " 1011,\n",
       " 5533,\n",
       " 8827,\n",
       " 17994,\n",
       " 9648,\n",
       " 10415,\n",
       " 2600,\n",
       " 2201,\n",
       " 2008,\n",
       " 2001,\n",
       " 8216,\n",
       " 2135,\n",
       " 19657,\n",
       " 2007,\n",
       " 1996,\n",
       " 2316,\n",
       " 1005,\n",
       " 1055,\n",
       " 4563,\n",
       " 5470,\n",
       " 15058,\n",
       " 1012,\n",
       " 102,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second[\"input_ids\"].tolist()[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5f43d272-a4a4-484b-bace-604d2b7b7972",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2016,\n",
       " 2003,\n",
       " 3862,\n",
       " 2005,\n",
       " 2195,\n",
       " 10106,\n",
       " 1010,\n",
       " 2164,\n",
       " 1996,\n",
       " 2088,\n",
       " 1011,\n",
       " 2898,\n",
       " 19855,\n",
       " 1004,\n",
       " 14783,\n",
       " 1058,\n",
       " 4877,\n",
       " 2072,\n",
       " 2640,\n",
       " 4329,\n",
       " 1010,\n",
       " 2029,\n",
       " 2016,\n",
       " 2318,\n",
       " 2007,\n",
       " 20163,\n",
       " 19855,\n",
       " 1011,\n",
       " 1037,\n",
       " 2088,\n",
       " 1011,\n",
       " 2898,\n",
       " 4297,\n",
       " 19761,\n",
       " 4263,\n",
       " 1997,\n",
       " 1996,\n",
       " 8361,\n",
       " 3968,\n",
       " 2050,\n",
       " 3068,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][\"input_ids\"][16:32][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "137af1ff-2d16-499b-8b98-3f26bf13eb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] she is notable for several achievements, including the world - wide mead & conway vlsi design revolution, which she started with carver mead - a world - wide incubator of the emerging eda industry. [SEP]'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_datasets[\"train\"][\"input_ids\"][16:32][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df431de6-4f63-4e50-87d8-2673bdf93857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6f56c5-47a1-4eec-87b1-e4d7e5b6551a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f04c0-8564-449c-b2ef-ce265df6d4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106bca6-380b-439f-8b08-820c633592bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "830a1a01-2e97-4805-a772-bfa9d5b2246c",
   "metadata": {},
   "source": [
    "## Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "445a1d3f-efa7-4d82-8be9-3329abb7e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1d5f7a-455c-4617-8750-e066065e2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e4e33b6-d9c6-40a1-8eff-fe01348ccb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/home/cdsw/models/bert-cls-full3/checkpoint-96000/\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd7a9e25-0f25-4f24-89c3-ad34866504d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(task=\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be70789c-2926-4f47-8dd3-7181ffda058e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.9783034920692444}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    \"following the end of kenneth kaunda's repressive dictatorship , chiluba won the country's multi-party presidential elections.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "074aa3c1-6afe-4a48-80fb-797367405780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_0', 'score': 0.981116771697998}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(\n",
    "    \"following the end of kenneth kaunda's repressive presidency , chiluba won the country's multi-party presidential elections.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d2f99ba-9e95-48a9-bd28-d2d993d33a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text-classification'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0ae3e3-e22b-40aa-8325-0b7dd36841cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "279c7d42-a02a-416b-a867-1bf79ece64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b6ae33c-3cea-4b8f-87de-01d7a217f006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'bertscore',\n",
       " 'bleu',\n",
       " 'bleurt',\n",
       " 'cer',\n",
       " 'chrf',\n",
       " 'code_eval',\n",
       " 'comet',\n",
       " 'competition_math',\n",
       " 'coval',\n",
       " 'cuad',\n",
       " 'exact_match',\n",
       " 'f1',\n",
       " 'frugalscore',\n",
       " 'glue',\n",
       " 'google_bleu',\n",
       " 'indic_glue',\n",
       " 'mae',\n",
       " 'mahalanobis',\n",
       " 'matthews_correlation',\n",
       " 'mauve',\n",
       " 'mean_iou',\n",
       " 'meteor',\n",
       " 'mse',\n",
       " 'pearsonr',\n",
       " 'perplexity',\n",
       " 'precision',\n",
       " 'recall',\n",
       " 'roc_auc',\n",
       " 'rouge',\n",
       " 'sacrebleu',\n",
       " 'sari',\n",
       " 'seqeval',\n",
       " 'spearmanr',\n",
       " 'squad',\n",
       " 'squad_v2',\n",
       " 'super_glue',\n",
       " 'ter',\n",
       " 'wer',\n",
       " 'wiki_split',\n",
       " 'xnli',\n",
       " 'xtreme_s']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.list_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "deee98d0-40f4-4736-86e0-b9a870f820c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.training_args.TrainingArguments"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b9f11b1-5913-49d5-9a8c-0cff1e768bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ae1fe-c614-4a86-8cbd-bfff30ff4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e532ed13-46ea-44e6-a8d9-3d9c9860c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from transformers.trainer_utils import IntervalStrategy\n",
    "\n",
    "@dataclass\n",
    "class StiArguments:\n",
    "    \"\"\"\n",
    "    TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop itself**.\n",
    "    Using [`HfArgumentParser`] we can turn this class into [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the command line.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    output_dir: str = field(\n",
    "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
    "    )\n",
    "    overwrite_output_dir: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Overwrite the content of the output directory. \"\n",
    "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n",
    "    per_device_train_batch_size: int = field(\n",
    "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n",
    "    )\n",
    "    per_device_eval_batch_size: int = field(\n",
    "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
    "    )\n",
    "    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n",
    "    logging_dir: Optional[str] = field(default=None, metadata={\"help\": \"Tensorboard log dir.\"})\n",
    "    logging_strategy: IntervalStrategy = field(\n",
    "        default=\"steps\",\n",
    "        metadata={\"help\": \"The logging strategy to use.\"},\n",
    "    )\n",
    "    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n",
    "    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n",
    "    evaluation_strategy: IntervalStrategy = field(\n",
    "        default=\"no\",\n",
    "        metadata={\"help\": \"The evaluation strategy to use.\"},\n",
    "    )\n",
    "    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
    "    save_total_limit: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Limit the total amount of checkpoints. \"\n",
    "                \"Deletes the older checkpoints in the output_dir. Default is unlimited checkpoints\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    load_best_model_at_end: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether or not to load the best model found during training at the end of training.\"},\n",
    "    )\n",
    "    metric_for_best_model: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The metric to use to compare two different models.\"}\n",
    "    )\n",
    "    greater_is_better: Optional[bool] = field(\n",
    "        default=None, metadata={\"help\": \"Whether the `metric_for_best_model` should be maximized or not.\"}\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a98874f-7784-41b7-aba7-46640e3ef687",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(StiArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f96f1fc-ba19-4859-8e30-362fa499bd22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HfArgumentParser(prog='ipykernel_launcher.py', usage=None, description=None, formatter_class=<class 'argparse.ArgumentDefaultsHelpFormatter'>, conflict_handler='error', add_help=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ca294ef3-5e8d-4197-894f-0928215be4c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__annotations__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_action_groups',\n",
       " '_actions',\n",
       " '_add_action',\n",
       " '_add_container_actions',\n",
       " '_add_dataclass_arguments',\n",
       " '_check_conflict',\n",
       " '_check_value',\n",
       " '_defaults',\n",
       " '_get_args',\n",
       " '_get_formatter',\n",
       " '_get_handler',\n",
       " '_get_kwargs',\n",
       " '_get_nargs_pattern',\n",
       " '_get_option_tuples',\n",
       " '_get_optional_actions',\n",
       " '_get_optional_kwargs',\n",
       " '_get_positional_actions',\n",
       " '_get_positional_kwargs',\n",
       " '_get_value',\n",
       " '_get_values',\n",
       " '_handle_conflict_error',\n",
       " '_handle_conflict_resolve',\n",
       " '_has_negative_number_optionals',\n",
       " '_match_argument',\n",
       " '_match_arguments_partial',\n",
       " '_mutually_exclusive_groups',\n",
       " '_negative_number_matcher',\n",
       " '_option_string_actions',\n",
       " '_optionals',\n",
       " '_parse_known_args',\n",
       " '_parse_optional',\n",
       " '_pop_action_class',\n",
       " '_positionals',\n",
       " '_print_message',\n",
       " '_read_args_from_files',\n",
       " '_registries',\n",
       " '_registry_get',\n",
       " '_remove_action',\n",
       " '_subparsers',\n",
       " 'add_argument',\n",
       " 'add_argument_group',\n",
       " 'add_help',\n",
       " 'add_mutually_exclusive_group',\n",
       " 'add_subparsers',\n",
       " 'allow_abbrev',\n",
       " 'argument_default',\n",
       " 'conflict_handler',\n",
       " 'convert_arg_line_to_args',\n",
       " 'dataclass_types',\n",
       " 'description',\n",
       " 'epilog',\n",
       " 'error',\n",
       " 'exit',\n",
       " 'exit_on_error',\n",
       " 'format_help',\n",
       " 'format_usage',\n",
       " 'formatter_class',\n",
       " 'fromfile_prefix_chars',\n",
       " 'get_default',\n",
       " 'parse_args',\n",
       " 'parse_args_into_dataclasses',\n",
       " 'parse_dict',\n",
       " 'parse_intermixed_args',\n",
       " 'parse_json_file',\n",
       " 'parse_known_args',\n",
       " 'parse_known_intermixed_args',\n",
       " 'prefix_chars',\n",
       " 'print_help',\n",
       " 'print_usage',\n",
       " 'prog',\n",
       " 'register',\n",
       " 'set_defaults',\n",
       " 'usage']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06de5ec8-a188-4209-b243-6b4068c659e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f61b7fef-84d4-428e-a5ea-1a85961982fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Script to run train job for seq2seq (TST) or classifier (STI) models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccab520-ac45-43fa-bc06-b03d588805a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.add_argument("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf0c666-caab-4e63-9160-49f10f0c61fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68b1b58b-88e8-474c-8be4-fea59d52683c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=[], dest='task', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help='Select which task to run: seq2seq or classifier.', metavar=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('task', type=str, help='Select which task to run: seq2seq or classifier.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d3b8379-6bd8-4426-a148-21615b5efa55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArgumentParser(prog='ipykernel_launcher.py', usage=None, description='Script to run train job for seq2seq (TST) or classifier (STI) models.', formatter_class=<class 'argparse.HelpFormatter'>, conflict_handler='error', add_help=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5595fac-0f48-40ea-8054-3ad6aee84f05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
