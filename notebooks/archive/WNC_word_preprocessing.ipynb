{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d7dcd3f-1864-4647-b7c6-3d0d928f3c35",
   "metadata": {},
   "source": [
    "# WNC One-word Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "781a90b8-567d-4a06-a726-5cbb404ba466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lab_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext lab_black\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "from operator import concat\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Value,\n",
    "    Translation,\n",
    "    Features,\n",
    "    load_from_disk,\n",
    ")\n",
    "\n",
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c35d4f-056d-4f6d-881a-9e78e46f86d7",
   "metadata": {},
   "source": [
    "## Construct HF Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b1f05e1-5cf2-4a29-9e7f-2d5a5c96f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hf_dataset(path: str) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Formats the raw biased-word data (train/dev/test) into a HuggingFace DatasetDict object.\n",
    "\n",
    "    Provided a path to the raw, one-word Wiki Neutrality Corpus (WNC) data files, this function parses\n",
    "    each of the dev, test, and train sets and formats them as a HuggingFace DatasetDict object\n",
    "    in the seq2seq style (i.e. ready for translation tasks).\n",
    "\n",
    "    For data, see -> https://arxiv.org/pdf/1911.09709.pdf\n",
    "\n",
    "    Args:\n",
    "        path (str): path to directory containing raw WNC data files\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    splits = [\"dev\", \"test\", \"train\"]\n",
    "    dataset_dict = defaultdict(dict)\n",
    "\n",
    "    FEATURES = Features(\n",
    "        {\n",
    "            \"rev_id\": Value(\"string\"),\n",
    "            \"translation\": Translation(languages=[\"pre\", \"post\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    for split in splits:\n",
    "\n",
    "        PATH = os.path.join(path, f\"biased.word.{split}\")\n",
    "\n",
    "        rev_ids = []\n",
    "        translation_pairs = []\n",
    "\n",
    "        for i, line in enumerate(tqdm(open(PATH))):\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "\n",
    "            # note some entries contain the POS and REL fields, others dont\n",
    "            if len(parts) == 7:\n",
    "                rev_id, pre_tok, post_tok, pre_raw, post_raw, pos, rels = parts\n",
    "\n",
    "            elif len(parts) == 5:\n",
    "                rev_id, pre_tok, post_tok, pre_raw, post_raw = parts\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipped entry: {i}\")\n",
    "\n",
    "            rev_ids.append(rev_id)\n",
    "            translation_pairs.append({\"pre\": pre_raw, \"post\": post_raw})\n",
    "\n",
    "        split_dict = {\n",
    "            \"rev_id\": rev_ids,\n",
    "            \"translation\": translation_pairs,\n",
    "        }\n",
    "\n",
    "        dataset_dict[split] = Dataset.from_dict(split_dict, features=FEATURES)\n",
    "\n",
    "    return DatasetDict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841d7c03-402d-4ddd-a502-adf7671fe31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "700it [00:00, 54657.05it/s]\n",
      "1000it [00:00, 50495.46it/s]\n",
      "53803it [00:00, 61367.11it/s]\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"/home/cdsw/data/raw/bias_data/WNC\"\n",
    "\n",
    "wnc_datasets = build_hf_dataset(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf53f3e-5c0f-483d-92d0-0c304400767a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    dev: Dataset({\n",
       "        features: ['rev_id', 'translation'],\n",
       "        num_rows: 700\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['rev_id', 'translation'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['rev_id', 'translation'],\n",
       "        num_rows: 53803\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb676b4-02b3-493e-aad6-34bb02b3f59e",
   "metadata": {},
   "source": [
    "## Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa31878-8860-4068-8030-1b6d2c3f3a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_by_revid(datasets: DatasetDict) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Remove duplicate records from datasets.\n",
    "\n",
    "    Provided a DatasetDict of the WNC, this function filters out all duplicate\n",
    "    records as defined by their rev_id.\n",
    "\n",
    "    Args:\n",
    "        datasets (DatasetDict): a WNC datasets object returned by `build_hf_dataset()`\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    rev_ids = reduce(concat, [datasets[split][\"rev_id\"] for split in datasets.keys()])\n",
    "\n",
    "    duplicate_revids = set([rev_id for rev_id in rev_ids if rev_ids.count(rev_id) > 1])\n",
    "\n",
    "    print(f\"{len(duplicate_revids)*2} duplicate records have been removed.\")\n",
    "\n",
    "    return datasets.filter(lambda x: x[\"rev_id\"] not in duplicate_revids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f9411d6-af5a-4482-bbd8-c9ebf3085636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 duplicate records have been removed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0c4ea8676e44645824471a6f672bd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4982e11119ca4c5fa3267e87e603a02c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140b2cc727c34b9ba672d067736dd91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wnc_datasets = remove_duplicate_by_revid(wnc_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a652a5-dd3a-4273-b451-26ba9d603351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    dev: Dataset({\n",
       "        features: ['rev_id', 'translation'],\n",
       "        num_rows: 700\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['rev_id', 'translation'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['rev_id', 'translation'],\n",
       "        num_rows: 53791\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53dfc51-b168-4c32-ae8e-b932a88d49c5",
   "metadata": {},
   "source": [
    "## Remove outlier translation pairs\n",
    "\n",
    "**Here, we define outliers by:**\n",
    "1. Records with pre-edit sentence length > 99th percentile.\n",
    "2. Records with pre-edit sentence length < 1st percentile.\n",
    "3. Records with net subtraction of more than 1 word. This is a \"one word edit\" version of the dataset so these are here in error. Manual inspections show these are in here due to grammer edits that occured along with NPOV edit. \n",
    "4. Records with net addition of more than 4 terms. This is based on manual inspection of the one-word version where additive edits up to net of 4 tokens seem to be mostly legitimate NPOV replacements, while those above 4 are rare and typically include a grammatical edit along with the NPOV edit (i.e. commas with extra spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5d18694-21a4-43b7-809b-4b67efd4b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(datasets: DatasetDict) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    Remove outlier observations from the datasets.\n",
    "\n",
    "    Provided a DatasetDict of the WNC, this function filters out all outlier\n",
    "    records as defined by:\n",
    "\n",
    "        1. Records with pre-edit sentence length > 99th percentile.\n",
    "        2. Records with pre-edit sentence length < 1st percentile.\n",
    "        3. Records with net subtraction of more than 1 word. This is a \"one word edit\"\n",
    "            version of the dataset so these are here in error. Manual inspections show\n",
    "            these are in here due to grammer edits that occured along with NPOV edit.\n",
    "        4. Records with net addition of more than 4 terms. This is based on manual\n",
    "            inspection of the one-word version where additive edits up to net of 4 tokens\n",
    "            seem to be mostly legitimate NPOV replacements, while those above 4 are rare\n",
    "            and typically include a grammatical edit along with the NPOV edit (i.e. commas\n",
    "            with extra spaces).\n",
    "\n",
    "    Args:\n",
    "        datasets (DatasetDict): a WNC datasets object returned by `build_hf_dataset()`\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # first create new columns with calculated sentence lengths and the difference\n",
    "    datasets = datasets.map(\n",
    "        lambda example: {\n",
    "            \"length_pre\": len(example[\"translation\"][\"pre\"].split()),\n",
    "            \"length_post\": len(example[\"translation\"][\"post\"].split()),\n",
    "        }\n",
    "    ).map(\n",
    "        lambda example: {\"length_delta\": example[\"length_post\"] - example[\"length_pre\"]}\n",
    "    )\n",
    "\n",
    "    # set length difference bounds determine sentence length cutoffs\n",
    "    subtractive_bound = -1\n",
    "    additive_bound = 4\n",
    "    lower_length, upper_length = (\n",
    "        datasets[\"train\"].to_pandas()[\"length_pre\"].quantile(q=[0.01, 0.99])\n",
    "    )\n",
    "\n",
    "    nrows_prior = datasets.num_rows\n",
    "\n",
    "    datasets = datasets.filter(\n",
    "        lambda example: example[\"length_pre\"] <= upper_length\n",
    "        and example[\"length_pre\"] >= lower_length\n",
    "        and example[\"length_delta\"] >= subtractive_bound\n",
    "        and example[\"length_delta\"] <= additive_bound\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"Number of records removed: { {k: nrows_prior[k] - v.num_rows for k, v in datasets.items()} }\"\n",
    "    )\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d61ea500-22c1-480e-891f-3f59e6b3d582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0eb07f6f90c4804ad067c3ee5ef992c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1557cd456b048e68f57bdb2b942202e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99d856ca587413dbee7451ef1d17f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34496844afd04046bb8711f60fe2a972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa5fcd7e5d54819901638d7c57b68a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bce7d44fce64705922d2dfab758a5c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ea6d94ae7e47a0b51a55170e5a6964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79e00bfa9c541a8a69e3313f2bef975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf166501f86c49ab91714bfa87024a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records removed: {'dev': 29, 'test': 33, 'train': 1698}\n"
     ]
    }
   ],
   "source": [
    "wnc_datasets = remove_outliers(wnc_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e44cb9e8-3bc3-403f-8595-76e023a4920f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    dev: Dataset({\n",
       "        features: ['rev_id', 'translation', 'length_pre', 'length_post', 'length_delta'],\n",
       "        num_rows: 671\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['rev_id', 'translation', 'length_pre', 'length_post', 'length_delta'],\n",
       "        num_rows: 967\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['rev_id', 'translation', 'length_pre', 'length_post', 'length_delta'],\n",
       "        num_rows: 52093\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ebffd-450a-4837-bf8f-4752df111a14",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d52624af-76eb-49f0-be2b-821138a70394",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnc_datasets_clean = load_from_disk(\"../data/processed/WNC-oneword\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "515bcf02-d064-4a0c-9c10-a9b76c748929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    dev: Dataset({\n",
       "        features: ['rev_id', 'translation'],\n",
       "        num_rows: 671\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['rev_id', 'translation'],\n",
       "        num_rows: 967\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['rev_id', 'translation'],\n",
       "        num_rows: 52093\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnc_datasets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2fa0bc-e9a5-44ba-bec3-68ff81f9bbea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
